"""
å…±ç”¨å‡½æ•°æˆ–å·¥å…·åº“
"""

# %%
import csv
import os
import queue
import re
import threading
from datetime import datetime
from time import time
from logging import debug, error, info
from typing import List, Optional, Union
from urllib.parse import unquote, urlparse
from pandas import Series
import pandas as pd
import requests
from bs4 import BeautifulSoup
import unicodedata

SUPPORT_IMAGE_FORMATS_NAME = (
    "jpg",
    "jpeg",
    "png",
    "webp",
    "heic",
    "tif",
    "tiff",
    "bmp",
    "gif",
    "avif",
)
SUPPORT_IMAGE_FORMATS = ("." + f for f in SUPPORT_IMAGE_FORMATS_NAME)
csv.field_size_limit(int(1e7))  # å…è®¸csvæ–‡ä»¶æœ€å¤§ä¸º10MB
# æœ‰äº›å›¾ç‰‡çš„urlä¸­å¯èƒ½åŒ…å«ç©ºæ ¼!
COMMON_SEPARATORS = [
    ",",
    ";",
    #  , r"\s+"
]
URL_SEPARATORS = [
    # r"\s+",
    ">",
    # ";",
    # ",",
]
URL_MAIN_DOMAIN_PATTERN = r"(?:https?://)?(?:[\w-]+\.)*([^/]+[.][^/]+)/?"
# (https?://)?([\w-]+\.)*([^/]+[.][^/]+)/?
URL_MAIN_DOMAIN_NAME_PATTERN = (
    r"(https?://)?([\w-]+\.)*(?P<main_domain>[^/]+[.][^/]+)/?"
)
URL_SEP_PATTERN = "|".join(URL_SEPARATORS)
COMMON_SEP_PATTERN = "|".join(COMMON_SEPARATORS)
EMAIL_PATTERN = r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}"
# æå–ç½‘å€çš„æ­£åˆ™è¡¨è¾¾å¼(éƒ¨åˆ†æƒ…å†µä¸‹urlä¸­åŒ…å«ç©ºç™½å­—ç¬¦(æ¯”å¦‚ç©ºæ ¼)),å¯¹äºé‡‡é›†å¤šä¸ªå›¾ç‰‡urlçš„æ—¶å€™æœ‰å…³
# HTTP_S_URL_CONS_PATTERN = r'https?://[^\s"<>]+'
HTTP_S_URL_CONS_PATTERN = r'https?://[^"<>]+'
URL_SEP_REGEXP = re.compile(URL_SEP_PATTERN)
COMMON_SEP_REGEXP = re.compile(COMMON_SEP_PATTERN)


def get_now_time_str():
    """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²(æ—¶åˆ†ç§’)
    é€‚åˆäºåˆ›å»ºæ–‡ä»¶åä½¿ç”¨,å¹´æœˆæ—¥--æ—¶åˆ†ç§’@æ—¶é—´æˆ³
    æ ¼å¼: 2023-07-05--12-34-56@
    """
    timestamp = int(time())
    return datetime.now().strftime("%Y-%m-%d--%H-%M-%S") + f"@{timestamp}"


def get_desktop_path():
    """è·å–å½“å‰ç”¨æˆ·æ¡Œé¢è·¯å¾„
    å°¤å…¶æ˜¯windowsç”¨æˆ·
    """
    return os.path.join(os.path.expanduser("~"), "Desktop")


def get_paths(input_dir: str, recurse: bool = False):
    """
    è·å–æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰è·¯å¾„(ç»å¯¹è·¯å¾„)
    å…¶ä¸­åŒ…æ‹¬äº†æ–‡ä»¶å’Œå­ç›®å½•

    Args:
        input_dir (str): è¦éå†çš„ç›®å½•è·¯å¾„ã€‚
        recurse (bool): æ˜¯å¦é€’å½’éå†å­ç›®å½•ï¼Œé»˜è®¤ä¸º Falseã€‚

    Returns:
        List[str]: æ‰€æœ‰æ–‡ä»¶çš„å®Œæ•´è·¯å¾„åˆ—è¡¨ã€‚
    """
    files = []

    if recurse:
        for root, _, fs in os.walk(input_dir):
            files.extend([os.path.join(root, filename) for filename in fs])
    else:
        files = [
            os.path.join(input_dir, filename) for filename in os.listdir(input_dir)
        ]

    return files


def sanitize_filename(filename, length_limit=200):
    """ç§»é™¤æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦,å¹¶æ‰§è¡ŒasciiåŒ–å’Œé•¿åº¦é™åˆ¶,æ˜¯çš„æ–‡ä»¶åä¸­çš„å„ä¸ªå­—ç¬¦éƒ½å…è®¸å‡ºç°åœ¨ç³»ç»Ÿæ–‡ä»¶åè§„èŒƒä¸­
    æ³¨æ„,asciiå­—ç¬¦ä¹Ÿä¸éƒ½èƒ½ä½œä¸ºæ–‡ä»¶å,å› æ­¤æœ€åä½¿ç”¨ä¸€ä¸ªæ­£åˆ™æ›¿æ¢å…œåº•
    åŸºæœ¬æ•ˆæœ:
    1.å…¨è§’->åŠè§’
    2.é‡éŸ³ç¬¦å·å»é™¤é‡éŸ³(æ¯”å¦‚Ã©)->e
    3.ç§»é™¤éASCIIå­—ç¬¦
    è¿™ä¸ªå¤„ç†å¯èƒ½ä½¿å¾—åŸæ¥ä¸€æ‰¹æ–‡ä»¶ä¸­çš„æ–‡ä»¶åçš„å”¯ä¸€æ€§ä¸¢å¤±
    å®é™…åº”ç”¨ä¸­,å¯ä»¥é…åˆæ·»åŠ åç¼€(ä¾‹å¦‚æ—¥æœŸ-æ—¶é—´)æ¥ä½¿æ–‡ä»¶åå…·æœ‰å”¯ä¸€æ€§

    Example:
    test="æµ‹è¯•æ–‡ä»¶ï¼¡ï¼¢ï¼£ cafÃ©ï¬.txt"
    print(sanitize_filename(test))
    """
    # 1. Unicodeæ ‡å‡†åŒ–
    filename = unicodedata.normalize("NFKD", filename)
    # print(f"NFKDæ ‡å‡†åŒ–å: {repr(filename)}")

    # 2. ç§»é™¤éASCIIå­—ç¬¦,é…åˆUnicodeæ ‡å‡†åŒ–,å¯ä»¥å»é™¤é‡éŸ³å­—ç¬¦çš„é‡éŸ³ç¬¦å·
    filename = filename.encode("ascii", "ignore").decode("ascii")

    # 3. æ›¿æ¢éæ³•å­—ç¬¦
    filename = re.sub(r"[^\w\-_.]", "_", filename)

    return filename[:length_limit]


def walk_with_depth(root_dir, depth=None):
    """
    é€’å½’éå†ç›®å½•ï¼Œæ”¯æŒæŒ‡å®šé€’å½’æ·±åº¦å’Œè¿‡æ»¤ç›®å½•/æ–‡ä»¶ã€‚

    Args:
        root_dir (str): æ ¹ç›®å½•è·¯å¾„ã€‚
        depth (int, optional): éå†çš„æœ€å¤§æ·±åº¦ï¼Œé»˜è®¤ä¸º Noneï¼ˆæ— é™åˆ¶ï¼‰ã€‚

    Example:
        >>> test_dir=r"C:/ShareTemp/imgs_demo"
        >>> walk_with_depth(test_dir,depth=1 )
    """

    dirs = []
    files = []

    def walker(path, current_depth):
        if depth is not None and current_depth > depth:
            return

        try:
            entries = os.listdir(path)
        except PermissionError:
            # å¿½ç•¥æ— æ³•è®¿é—®çš„ç›®å½•
            return

        for entry in entries:
            full_path = os.path.join(path, entry)

            if os.path.isdir(full_path):
                dirs.append(full_path)
                walker(full_path, current_depth + 1)
            else:
                files.append(full_path)

    walker(root_dir, 1)
    return dirs, files


def merge_table_files(
    directory: str, out_file="", remove_old_files=False, encoding: str = "utf-8"
) -> pd.DataFrame:
    """
    è¯»å–æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ CSV æ–‡ä»¶å¹¶åˆå¹¶ä¸ºä¸€ä¸ªç»“æ„ç»Ÿä¸€çš„ DataFrameã€‚
    å¦‚æœæœ‰ç»“æ„ç›¸åŒçš„excelè¡¨æ ¼.xlsx(xls)æˆ–è€…csv,excelè¡¨æ ¼æ–‡ä»¶æ··åˆå­˜æ”¾ä½†æ˜¯è¡¨å¤´ä¸€æ ·,ä¹Ÿå¯ä»¥è¯»å–å¹¶åˆå¹¶

    Args:
        directory (str): å­˜æ”¾ CSV æ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚
        out_file (str, optional): åˆå¹¶åçš„ CSV æ–‡ä»¶çš„è¾“å‡ºè·¯å¾„ï¼Œç•™ç©ºåˆ™ä¸è¾“å‡ºã€‚
        remove_old_files (bool, optional): æ˜¯å¦åˆ é™¤åŸæœ‰ CSV æ–‡ä»¶ï¼Œé»˜è®¤ä¸º Trueã€‚
        encoding (str, optional): CSV æ–‡ä»¶çš„ç¼–ç æ ¼å¼ï¼Œé»˜è®¤ä¸º 'utf-8'ã€‚

    Returns:
        pd.DataFrame: åˆå¹¶åçš„ DataFrameã€‚å¦‚æœç›®å½•ä¸­æ²¡æœ‰ CSV æ–‡ä»¶ï¼Œåˆ™è¿”å›ä¸€ä¸ªç©ºçš„ DataFrameã€‚

    Examples:
        >>> merged_df = merge_table_files(r'./csv_demo')
        >>> print(merged_df)
    """
    os.makedirs(directory, exist_ok=True)
    # è·å–æ‰€æœ‰ .csv æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    table_files = [
        os.path.join(directory, file)
        for file in os.listdir(directory)
        if file.endswith(".csv") or file.endswith(".xlsx") or file.endswith(".xls")
    ]

    if not table_files:
        return pd.DataFrame()

    # è¯»å–æ‰€æœ‰ CSV æ–‡ä»¶åˆ° DataFrame åˆ—è¡¨
    # dfs = [pd.read_csv(file, encoding=encoding) for file in csv_files]
    dfs = [read_table_data(file, encoding=encoding) for file in table_files]

    if remove_old_files:
        for file in table_files:
            os.remove(file)
    # åˆå¹¶æ‰€æœ‰çš„ DataFrame
    merged_df = pd.concat(dfs, ignore_index=True)
    if out_file:
        merged_df.to_csv(out_file, index=False, encoding=encoding)

    return merged_df


def remove_duplicate_rows(file, subset=None, inplace=True):
    """ç§»é™¤csvæ–‡ä»¶ä¸­çš„é‡å¤è¡Œ,é»˜è®¤ç›´æ¥ä¿®æ”¹åŸæ–‡ä»¶
    ä¾‹å¦‚skué‡å¤çš„è¡Œï¼Œåªä¿ç•™ä¸€æ¡

    Args:
        csv_file (str): å¾…å¤„ç†çš„ CSV æ–‡ä»¶è·¯å¾„ã€‚
        inplace (bool, optional): æ˜¯å¦ç›´æ¥ä¿®æ”¹åŸæ–‡ä»¶ï¼Œé»˜è®¤ä¸º Trueã€‚

    """
    print(f"remove duplicate rows in subset [{subset}] from {file}")
    df = pd.DataFrame()
    if os.path.exists(file):
        df = pd.read_csv(file)
        df.drop_duplicates(subset=subset, inplace=inplace)
        df.to_csv(file, index=False)
    else:
        print(f"{file} not exists")
    return df


def merge_csv_naive(csv_dir, out_file="", remove_old_files=False):
    """è¯»å–æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰csvæ–‡ä»¶ï¼Œå¹¶åˆå¹¶æˆä¸€ä¸ªcsvæ–‡ä»¶
    æ³¨æ„,å¦‚æœcsvçš„æ ¼å¼ä¸åŒ(æ¯”å¦‚å…·æœ‰ä¸åŒçš„åˆ—å,æ— æ³•ä½¿ç”¨æ­¤å‡½æ•°åˆå¹¶)
    ä»…ä½¿ç”¨pythonè‡ªå¸¦çš„csvæ¨¡å—,è€Œä¸ä¾èµ–äºpandas

    Args:
        csv_dir (str): csvæ–‡ä»¶æ‰€åœ¨ç›®å½•
        out_file  : æ˜¯å¦å°†åˆå¹¶çš„æ•°æ®è¾“å‡ºæˆæ–°çš„csvæ–‡ä»¶,å‚æ•°ä¸ºéç©ºæ–‡ä»¶åæ—¶è¾“å‡ºåˆ°æ–‡ä»¶,å¦åˆ™ä¸è¾“å‡ºåˆ°æ–‡ä»¶
        remove_old_files: æ˜¯å¦åˆ é™¤åŸæœ‰csvæ–‡ä»¶,é»˜è®¤åˆ é™¤

    Returns:
        tuple: åˆå¹¶åçš„csvæ–‡ä»¶çš„æ ‡å¤´è¡Œå’Œæ•°æ®è¡Œæ„æˆçš„å…ƒç»„

    """
    lines = []
    fieldnames = []
    csv_files = [
        os.path.join(csv_dir, f) for f in os.listdir(csv_dir) if f.endswith(".csv")
    ]
    for csv_file in csv_files:
        with open(csv_file, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            fieldnames = reader.fieldnames
            for row in reader:
                lines.append(row)
    if remove_old_files:
        for csv_file in csv_files:
            os.remove(csv_file)
    if out_file:
        with open(
            os.path.join(csv_dir, out_file), "w", encoding="utf-8", newline=""
        ) as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames or [])
            writer.writeheader()
            for row in lines:
                writer.writerow(row)
        # return os.path.join(csv_dir, )

    return fieldnames, lines


def remove_empty_html_tags(html: str, tags=None) -> str:
    """
    ç§»é™¤ HTML ä¸­æ— å†…å®¹çš„æ ‡ç­¾ï¼ˆå¦‚ <span></span> æˆ–ä»…åŒ…å«ç©ºç™½å­—ç¬¦çš„æ ‡ç­¾å¯¹ï¼‰ã€‚
    å¯æŒ‡å®šè¦å¤„ç†çš„æ ‡ç­¾ç±»å‹ï¼Œé»˜è®¤å¤„ç†æ‰€æœ‰æ ‡ç­¾ã€‚

    Args:
        html (str): è¾“å…¥çš„ HTML å­—ç¬¦ä¸²ã€‚
        tags (list, optional): éœ€è¦å¤„ç†çš„æ ‡ç­¾ååˆ—è¡¨ï¼ˆå¦‚ ['span', 'div']ï¼‰ï¼Œ
            è‹¥ä¸º Noneï¼Œåˆ™å¤„ç†æ‰€æœ‰æ ‡ç­¾ã€‚

    Returns:
        str: å¤„ç†åçš„ HTML å­—ç¬¦ä¸²ã€‚
    """
    if not html:
        return html

    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ ‡ç­¾ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªé€šç”¨æ¨¡å¼åŒ¹é…æ‰€æœ‰æ ‡ç­¾
    if tags is None:
        # åŒ¹é…æ‰€æœ‰ç©ºæ ‡ç­¾æˆ–åªåŒ…å«ç©ºç™½å­—ç¬¦çš„æ ‡ç­¾
        pattern = r"<([a-zA-Z][a-zA-Z0-9]*)[^>]*>\s*</\1>"
    else:
        # åŒ¹é…æŒ‡å®šæ ‡ç­¾åˆ—è¡¨ä¸­çš„ç©ºæ ‡ç­¾
        tag_pattern = "|".join(tags)
        pattern = rf"<({tag_pattern})[^>]*>\s*</\1>"

    # æŒç»­æ›¿æ¢ç›´åˆ°æ²¡æœ‰æ›´å¤šåŒ¹é…é¡¹
    prev_html = ""
    current_html = html

    while prev_html != current_html:
        prev_html = current_html
        current_html = re.sub(pattern, "", current_html)

    return current_html


def remove_empty_html_tags_bybs(html: str, tags=None) -> str:
    """
    ç§»é™¤ HTML ä¸­æ— å†…å®¹çš„æ ‡ç­¾ï¼ˆå¦‚ <span></span> æˆ–ä»…åŒ…å«ç©ºç™½å­—ç¬¦çš„æ ‡ç­¾å¯¹ï¼‰ã€‚
    å¯æŒ‡å®šè¦å¤„ç†çš„æ ‡ç­¾ç±»å‹ï¼Œé»˜è®¤å¤„ç†æ‰€æœ‰æ ‡ç­¾ã€‚

    ä¾èµ–äº BeautifulSoup åº“

    Args:
        html (str): è¾“å…¥çš„ HTML å­—ç¬¦ä¸²ã€‚
        tags (list, optional): éœ€è¦å¤„ç†çš„æ ‡ç­¾ååˆ—è¡¨ï¼ˆå¦‚ ['span', 'div']ï¼‰ï¼Œ
            è‹¥ä¸º Noneï¼Œåˆ™å¤„ç†æ‰€æœ‰æ ‡ç­¾ã€‚

    Returns:
        str: å¤„ç†åçš„ HTML å­—ç¬¦ä¸²ã€‚

    Examples:
    >>> test_html = '<li><span class="x">   </span>   <div> <div>Detail</div> <div> <span> </span> <span> </span> </div> </div> <div><div> <p>No buckle</li> <li>Adjustable from 30 to 52</li> </ul> <p><b>FABRIC TECH:</b></p> <p>100% nylon</p> <p>ITEM #: PG-13736</p> </div></div> </li> <li> <div> <br>
    '
    >>> remove_empty_html_tags_bybs(test_html)

    """

    if not html:
        return html

    # ä½¿ç”¨ BeautifulSoup è§£æ HTML
    soup = BeautifulSoup(html, "html.parser")

    # æŒç»­æŸ¥æ‰¾å¹¶ç§»é™¤ç©ºæ ‡ç­¾ï¼Œç›´åˆ°æ²¡æœ‰æ›´å¤šå¯ç§»é™¤çš„æ ‡ç­¾
    changed = True
    while changed:
        changed = False

        # ç¡®å®šè¦å¤„ç†çš„æ ‡ç­¾
        elements = soup.find_all(tags) if tags else soup.find_all()

        for element in elements:
            # æ£€æŸ¥å…ƒç´ æ˜¯å¦ä¸ºç©ºï¼ˆåªåŒ…å«ç©ºç™½å­—ç¬¦ï¼‰
            if (
                element.name
                and (not element.get_text(strip=True))
                and not element.find_all()
            ):
                element.extract()  # ç§»é™¤ç©ºå…ƒç´ 
                changed = True

    return str(soup)


def get_user_choice_csv_fields(selected_ids, reader_headers):
    """è¯¢é—®ç”¨æˆ·é€‰æ‹©csvæ–‡ä»¶ä¸­çš„è¦é€‰æ‹©csvä¸­çš„å“ªäº›åˆ—
    è¯»å–å¹¶æ‰“å°csvçš„è¡¨å¤´,åˆ—å‡ºä¾›ç”¨æˆ·é€‰æ‹©,å¹¶è¿”å›åˆ—å·åˆ—è¡¨

    :param selected_ids: å·²ç»è®°ä½çš„åˆ—å·åˆ—è¡¨
    :param reader_headers: csvæ–‡ä»¶ä¸­çš„è¡¨å¤´
    :return: é€‰æ‹©çš„åˆ—å·åˆ—è¡¨

    """
    for idx, header in enumerate(reader_headers):
        print(f"{idx}. {header}")

    ipt = input(
        "è¯·è¾“å…¥éœ€è¦ä¸‹è½½çš„å›¾ç‰‡çš„åˆ—å·,å¦‚æœåªæœ‰ä¸€åˆ—,åˆ™è§†ä¸ºå›¾ç‰‡urlåˆ—,"
        "å¦‚æœæŒ‡å®šä¸¤åˆ—,åˆ™ç¬¬ä¸€åˆ—è§†ä¸ºæŒ‡å®šçš„ä¸‹è½½çš„æ–‡ä»¶ä¿å­˜çš„åå­—,ç¬¬äºŒåˆ—ä¸ºéœ€è¦ä¸‹è½½çš„URL,è¾“å…¥çš„åˆ—å·é—´ç”¨ç©ºæ ¼/é€—å·éš”å¼€: "
    )
    selectedx = re.split(COMMON_SEP_PATTERN, ipt.strip())
    if isinstance(selected_ids, list):
        selected_ids += selectedx
    info("You selected: %s", selectedx)
    return selected_ids


def split_multi(
    s: str,
    seps: Optional[Union[str, List[str]]] = None,
    ignore_empty: bool = True,
    strip_items: bool = True,
    case_sensitive: bool = True,
) -> List[str]:
    """
    æ ¹æ®å¤šä¸ªåˆ†éš”ç¬¦åˆ†å‰²å­—ç¬¦ä¸²ï¼Œæ”¯æŒçµæ´»å‚æ•°æ§åˆ¶ã€‚

    Args:
        s (str): å¾…åˆ†å‰²çš„å­—ç¬¦ä¸²ã€‚
        seps (str | List[str] | None): åˆ†éš”ç¬¦å­—ç¬¦ä¸²æˆ–åˆ†éš”ç¬¦åˆ—è¡¨ã€‚
            - None: é»˜è®¤ä½¿ç”¨é€—å·ã€åˆ†å·ã€å†’å·å’Œä»»æ„ç©ºç™½ç¬¦ã€‚
            - str: å•ä¸ªåˆ†éš”ç¬¦æˆ–æ­£åˆ™è¡¨è¾¾å¼ã€‚
            - List[str]: å¤šä¸ªåˆ†éš”ç¬¦ï¼Œå°†è‡ªåŠ¨ç»„åˆä¸ºæ­£åˆ™è¡¨è¾¾å¼ã€‚
        ignore_empty (bool): æ˜¯å¦å¿½ç•¥åˆ†å‰²åå¾—åˆ°çš„ç©ºå­—ç¬¦ä¸²ï¼Œé»˜è®¤Trueã€‚
        strip_items (bool): æ˜¯å¦å»é™¤æ¯ä¸ªåˆ†å‰²é¡¹çš„é¦–å°¾ç©ºç™½ï¼Œé»˜è®¤Trueã€‚
        case_sensitive (bool): åˆ†éš”ç¬¦æ˜¯å¦åŒºåˆ†å¤§å°å†™ï¼Œé»˜è®¤Trueã€‚

    Returns:
        List[str]: åˆ†å‰²åçš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚

    Examples:
        >>> split_multi("a, b; c :d   e")
        ['a', 'b', 'c', 'd', 'e']

        >>> split_multi("a|b|c", seps="|")
        ['a', 'b', 'c']


        >>> split_multi("a, b; c :d   e", seps=[",", ";", ":"," "], ignore_empty=True)
        ['a', 'b', 'c', 'd', 'e']

        >>> split_multi("a, b; c :d   e", seps=[",", ";", ":"," "], ignore_empty=False)
        ['a', '', 'b', '', 'c', '', 'd', '', '', 'e']

        >>> split_multi("a,,b", seps=",", ignore_empty=False)
        ['a', '', 'b']

        >>> split_multi("A,B,a", seps="a", case_sensitive=False)
        ['','B,','']

    """
    if seps is None:
        # é»˜è®¤åˆ†éš”ç¬¦ï¼šé€—å·ã€åˆ†å·ã€å†’å·ã€ä»»æ„ç©ºç™½ç¬¦
        pattern = r"[,\s;:]+"
    elif isinstance(seps, str):
        pattern = seps if len(seps) > 1 and seps.startswith("[") else re.escape(seps)
    elif isinstance(seps, list):
        pattern = "|".join(re.escape(sep) for sep in seps)
    else:
        raise ValueError("seps å‚æ•°å¿…é¡»ä¸º Noneã€str æˆ– List[str]")

    flags = 0 if case_sensitive else re.IGNORECASE
    items = re.split(pattern, s, flags=flags)
    if strip_items:
        items = [item.strip() for item in items]
    if ignore_empty:
        items = [item for item in items if item]
    return items


def get_main_domain_name_from_str(url, normalize=True):
    """
        ä»å­—ç¬¦ä¸²ä¸­æå–åŸŸå,ç»“æ„å½¢å¦‚ "äºŒçº§åŸŸå.é¡¶çº§åŸŸå",å³SLD.TLD;
        å¯¹äºæå–éƒ¨åˆ†çš„æ­£åˆ™,å¦‚æœå…è®¸è‹±æ–‡"å­—æ¯,-,æ•°å­—")(å¯¹äºç®€å•å®¹å¿å…¶ä»–å­—ç¬¦çš„åŸŸå,ä½¿ç”¨([^/]+)ä»£æ›¿([\\w]+)è¿™ä¸ªéƒ¨åˆ†

        ä»…æå–ä¸€ä¸ªåŸŸå,é€‚åˆäºå¯¹äºä¸€ä¸ªå­—ç¬¦ä¸²ä¸­ä»…åŒ…å«ä¸€ä¸ªç¡®å®šçš„åŸŸåçš„æƒ…å†µ
        ä¾‹å¦‚,å¯¹äºæ›´é•¿çš„ç»“æ„,"å­åŸŸå.äºŒçº§åŸŸå.é¡¶çº§åŸŸå"åˆ™ä¼šä¸¢å¼ƒå­åŸŸå,å‰ç¼€å¸¦æœ‰http(s)çš„éƒ¨åˆ†ä¹Ÿä¼šè¢«ç§»é™¤

        Args:
            url (str): å¾…å¤„ç†çš„URLå­—ç¬¦ä¸²
            normalize (bool, optional): æ˜¯å¦è¿›è¡Œè§„èŒƒåŒ–å¤„ç†(ç§»é™¤ç©ºæ ¼,å¹¶ä¸”å°†å­—æ¯å°å†™åŒ–å¤„ç†). Defaults to True.


        Examples:
# æµ‹è¯•URLåˆ—è¡¨
urls = [
    "www.domain1.com",
    "domain--name.com",
    "https://www.dom-ain2.com",
    "https://sports.whh3.cn.com",
    "domain-test4.com",
    "http://domain5.com",
    "https://domain6.com/",
    "# https://domain7.com",
    "http://",
    "https://www8./",
    "https:/www9",
]
for url in urls:
    domain = get_main_domain_name_from_str(url)
    print(domain)

# END
    """
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–åŸŸå
    url = str(url)
    # æ¸…ç†å¸¸è§çš„æ— æ•ˆurléƒ¨åˆ†
    url = re.sub(r"https?:/*w*\.?/?", "", url)
    # å°è¯•æå–è‹±æ–‡åŸŸå(æ³¨æ„,\wåŒ¹é…æ•°å­—,å­—æ¯,ä¸‹åˆ’çº¿,ä½†ä¸åŒ…æ‹¬ä¸­åˆ’çº¿,è€ŒåŸŸåä¸­å…è®¸,å› æ­¤è¿™é‡Œä½¿ç”¨[-\w+]è¡¨ç¤ºåŸŸåä¸­çš„å¯èƒ½çš„å­—ç¬¦(å°æ•°ç‚¹.æ¯”è¾ƒç‰¹æ®Š,å•ç‹¬å¤„ç†)
    match = re.search(r"(?:https?://)?(?:www\.)?(([-\w+]+\.)+[-\w+]+)", url)
    if match:
        res = match.group(1).strip("/")
        if normalize:
            # å­—æ¯å°å†™å¹¶ä¸”ç§»é™¤ç©ºç™½
            res = re.sub(r"\s+", "", res).lower()
        return res
    return ""


def set_image_extension(
    series: Series,
    default_image_format=".webp",
    supported_image_formats=SUPPORT_IMAGE_FORMATS_NAME,
):
    """
    æ‰¹é‡è®¾ç½®å›¾ç‰‡å­—æ®µåˆ—çš„å›¾ç‰‡æ ¼å¼
    å¦‚æœimageå­—æ®µåˆ—ä¸­çš„æ–‡ä»¶åå­—ç¬¦ä¸²ä»¥å¸¸è§çš„å›¾ç‰‡æ ¼å¼ç»“å°¾,åˆ™ä¸¢å¼ƒåç¼€ä»…ä¿ç•™æ–‡ä»¶å,å¦åˆ™è®¤ä¸ºè¿™ä¸ªå­—ç¬¦ä¸²ä¸å¸¦æ ¼å¼æ‰©å±•å,ä¿ç•™åŸæ ·

    æ­¤å‡½æ•°é’ˆå¯¹äºæ–‡ä»¶åä¸­åŒ…å«`.`ä½†æ˜¯åé¢è·Ÿéšçš„å¹¶ä¸æ˜¯æ–‡ä»¶æ ¼å¼(å°¤å…¶æ˜¯å›¾ç‰‡çš„åˆ¤æ–­)çš„æƒ…å†µ
    ä¾‹å¦‚æŸä¸ªå›¾ç‰‡çš„æ–‡ä»¶åæ˜¯`123.fieldskeepit`,è¿™æ—¶å€™å¦‚æœç›´æ¥ç”¨`os.path.splitext`è¿™ç§æ–¹æ³•ä¼šå°†`fieldskeepit`ä½œä¸ºæ–‡ä»¶æ ¼å¼,ç„¶è€Œè¿™ä¸æ˜¯ä¸€ä¸ªæ ¼å¼åå­—(æ‰©å±•å)
    é’ˆå¯¹å›¾ç‰‡æ–‡ä»¶,è®¸å¤šå›¾ç‰‡æ²¡æœ‰ç»™å‡ºæ–‡ä»¶æ ¼å¼,è¿™æ—¶å€™ä½¿ç”¨æ­¤å‡½æ•°å¯ä»¥å°†å¤§éƒ¨åˆ†æƒ…å†µåšæ­£ç¡®çš„å¤„ç†å¾—åˆ°ä¸å¸¦æ ¼å¼çš„å›¾ç‰‡å(è¿™ä¾èµ–äºsupported_image_formatsçš„é…ç½®çš„å®Œå–„ç¨‹åº¦)

    é€šè¿‡æŒ‡å®šé»˜è®¤æ ¼å¼,å¯ä»¥ä¸ºå›¾ç‰‡æ–‡ä»¶æŒ‡å®šä¸€ä¸ªé»˜è®¤æ ¼å¼(ç°åœ¨çš„å›¾ç‰‡è½¯ä»¶å’Œæµè§ˆå™¨åŸºæœ¬éƒ½èƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å›¾ç‰‡çœŸå®æ ¼å¼å¹¶æ¸²æŸ“,å› æ­¤åç¼€åå’Œå›¾ç‰‡å®é™…ç¼–ç æ ¼å¼å¯¹ä¸ä¸Šå¾€å¾€ä¸å½±å“æ˜¾ç¤º),è¿™å¯¹äºæŸäº›ä¸šåŠ¡æ˜¯å¾ˆæœ‰ç”¨çš„,ç®€å•æœ‰æ•ˆ

    :param series: å›¾ç‰‡å­—æ®µåˆ—
    :param default_image_format: é»˜è®¤å›¾ç‰‡æ ¼å¼(ä¾‹å¦‚'.webp',æ³¨æ„`.`å·)
    :param supported_image_formats: æ”¯æŒçš„å›¾ç‰‡æ ¼å¼åˆ—è¡¨
    :return: å¤„ç†åçš„å›¾ç‰‡å­—æ®µåˆ—serieså¯¹è±¡

    """

    res = (
        series.astype(str).apply(get_image_filebasename(supported_image_formats))
        + f"{default_image_format}"
    )

    return res


# è¯»å–è¡¨æ ¼æ•°æ®(ä»excel æˆ– csv æ–‡ä»¶æ–‡ä»¶è¯»å–æ•°æ®)
def read_table_data(file_path, encoding="utf-8"):
    """è¯»å–æ•°æ®
    æ ¹æ®æ–‡ä»¶ååˆ¤æ–­ä½¿ç”¨pd.csvè¿˜æ˜¯pd.exceläº¦æˆ–æ˜¯æ™®é€šçš„.confæ–‡ä»¶ç­‰å…¶ä»–æ™®é€šæ–‡æœ¬æ–‡ä»¶

    Args:
        file_path (str): æ–‡ä»¶è·¯å¾„
        encoding (str, optional): æ–‡ä»¶ç¼–ç . Defaults to "utf-8".å¯¹csvæ–‡ä»¶æœ‰æ•ˆ,excelæ–‡ä»¶è¯»å–æ— æ­¤å‚æ•°
    Returns:
        pd.DataFrame: æ•°æ®
    """
    df = pd.DataFrame()
    if file_path.endswith(".csv"):
        df = pd.read_csv(file_path, encoding=encoding)
    elif file_path.endswith(".xlsx") or file_path.endswith(".xls"):
        df = pd.read_excel(file_path)
    else:
        raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
    return df


def read_table(file_path, header=0, encoding=None, default_columns=None):
    """è¯»å–è¡¨æ ¼æ•°æ®,æ ¹æ®æ–‡ä»¶åç¼€è¯»å–æ–‡ä»¶(csv,xlsx,xls)

    Args:
        file_path (str): æ–‡ä»¶è·¯å¾„
        header (int, optional): è¡¨å¤´è¡Œæ•°. Defaults to 0.
        encoding (str, optional): æ–‡ä»¶ç¼–ç . å¯¹csvæ–‡ä»¶æœ‰æ•ˆ,excelæ–‡ä»¶è¯»å–æ— æ­¤å‚æ•°
            é»˜è®¤å°è¯•å¤šä¸ªç¼–ç (gbk,utf-8,gb2312),ä¹Ÿå¯ä»¥æŒ‡å®šç¼–ç (ä¸æ¨è)
        default_columns (list, optional): é»˜è®¤åˆ—å. å¦‚æœè¦è¯»å–çš„æ–‡ä»¶ä¸å­˜åœ¨,åˆ™æ ¹æ®æ­¤å‚æ•°åˆ—å‡ºçš„è¡¨å¤´æ„é€ ä¸€ä¸ªä»…æœ‰è¡¨å¤´çš„dataframe. Defaults to None.

    Returns:
        pd.DataFrame: æ•°æ®

    Examples:
        read_table(f"ab.csv", header=0,default_columns=['domain','name'])

    """
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨,åˆ™è¿”å›æŒ‡å®šè¡¨å¤´çš„dataframe
    if not os.path.exists(file_path):
        if default_columns:
            return pd.DataFrame(columns=default_columns)
        else:
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    if file_path.endswith(".csv"):
        # å¯¹äºcsvæ–‡ä»¶,æœ‰ä¸åŒç¼–ç æƒ…å†µ,å¦‚æœgbkè¯»å–å¤±è´¥,å°è¯•utf-8,gb2312
        if encoding:
            try:
                df = pd.read_csv(file_path, encoding=encoding, header=header)
            except UnicodeDecodeError as e:
                raise ValueError(
                    f"csvæ–‡ä»¶è¯»å–å¤±è´¥: ä½¿ç”¨æŒ‡å®šç¼–ç {encoding}è¯»å–å¤±è´¥"
                ) from e
        else:
            enc_candidates = ["utf-8", "gbk", "gb2312"]
            last_exc = None
            for enc in enc_candidates:
                try:
                    df = pd.read_csv(file_path, encoding=enc, header=header)
                    last_exc = None
                    break
                except UnicodeDecodeError as e:
                    # æ•è·é”™è¯¯å¹¶ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªç¼–ç 
                    last_exc = e
            if last_exc is not None:
                raise ValueError("csvæ–‡ä»¶è¯»å–å¤±è´¥: æ— æ³•è¯†åˆ«æ–‡ä»¶ç¼–ç ") from last_exc

    elif file_path.endswith((".xlsx", ".xls")):
        df = pd.read_excel(file_path, header=header)
    else:
        raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
    return df


def get_image_filebasename(supported_image_formats=SUPPORT_IMAGE_FORMATS_NAME):
    """å¾—åˆ°ä¸å¸¦æ ¼å¼çš„å›¾ç‰‡å
    è¿™ä¾èµ–äºsupported_image_formatsçš„é…ç½®çš„å®Œå–„ç¨‹åº¦

    :param supported_image_formats: æ”¯æŒçš„å›¾ç‰‡æ ¼å¼åˆ—è¡¨
    :return: ç”¨æ¥è®¡ç®—ä¸å¸¦æ ¼å¼æ‰©å±•åå›¾ç‰‡åçš„lambdaå‡½æ•°,å…¶å‚æ•°å¿…é¡»æ˜¯å­—ç¬¦ä¸²,å¦åˆ™æŠ›å‡ºå¼‚å¸¸(æ¯”å¦‚xæ˜¯ä¸ªæµ®ç‚¹æ•°,å°±ä¼šå› ä¸ºæ²¡æœ‰splitæ–¹æ³•æŠ¥é”™)

    Examples:
        from comutils import get_image_filebasename
        >>> get_image_filebasename()('abc.jpg')
        'abc'
        >>> get_image_filebasename()('abc.xxx')
        'abc.xxx'
        >>> get_image_filebasename(['png', 'jpg'])('abc.png')
        'abc'
        >>> get_image_filebasename(['png', 'jpg'])('abc.png.jpg')
        'abc.png'
    """

    return lambda x: (
        x.rsplit(".", 1)[0]
        if x.split(".")[-1].lower() in supported_image_formats
        else x
    )


def get_filebasename_from_url(url):
    """ä»URLä¸­æå–æ–‡ä»¶å(basename with extension)
    Args:
        url: è¦è¢«è§£æçš„URLæˆ–æ–‡ä»¶è·¯å¾„
    ä¾‹å¦‚: https://www.example.com/file.txt -> file.txt

    http://shopunitedgoods.com/ddcks%252000013__22568.1681078784.386.513.webp

    """
    parsed_url = urlparse(url)
    path = unquote(parsed_url.path)
    filename = os.path.basename(path)
    return filename


def complete_image_file_extension(
    file,
    default_extension="",
    supported_image_formats_name=SUPPORT_IMAGE_FORMATS_NAME,
    force_default_fmt=False,
):
    """è¡¥å…¨æ–‡ä»¶åå­—ç¬¦ä¸²çš„å›¾ç‰‡æ ¼å¼æ‰©å±•å
    å¦‚æœæ–‡ä»¶æœ¬èº«æœ‰æ‰©å±•å,åˆ™ä¸åšå¤„ç†
    å¦‚æœæ–‡ä»¶æœ¬èº«æ²¡æœ‰æ‰©å±•å,ä¸”æŒ‡å®šäº†é»˜è®¤æ‰©å±•å,å¦åˆ™ä¸ºå…¶é…ç½®æŒ‡å®šçš„æ‰©å±•å

    Args:
        file: æ–‡ä»¶åå­—ç¬¦ä¸²
        default_extension: é»˜è®¤æ‰©å±•å
        supported_image_formats: æ”¯æŒçš„å›¾ç‰‡æ ¼å¼åˆ—è¡¨

    Examples:
        >>> complete_image_file_extension("abc")
        'abc'
        >>> complete_image_file_extension("abc.png")
        'abc.png'
        >>> complete_image_file_extension("abc.jpg",default_extension=".webp")
        'abc.jpg'
        >>> complete_image_file_extension("abc.jpg",default_extension=".webp",force_fmt=True)
        'abc.webp'
        >>> complete_image_file_extension("abcjpg",default_extension=".webp")
        'abcjpg.webp'

    """
    supported_image_formats = tuple(("." + f for f in supported_image_formats_name))
    if file.endswith(supported_image_formats) and not force_default_fmt:
        return file
    else:
        return (
            get_image_filebasename(supported_image_formats_name)(file)
            + default_extension
        )


def extract_secondary_domain(url):
    """
    æ ¹æ®æä¾›çš„url,æˆ–è€…åŸŸå,æå–äºŒçº§åŸŸå

    Args:
        url (str): å¾…å¤„ç†çš„URLæˆ–åŸŸå

    Returns:
        str: æå–çš„äºŒçº§åŸŸåï¼Œå¦‚æœæå–å¤±è´¥åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²

    Raises:
        None

    Example:
        >>> extract_secondary_domain('https://www.example.com/path/to/page.html')
        'example.com'
        >>> extract_secondary_domain('https://www.example.co.uk/')
        'co.uk'
        >>> extract_secondary_domain('https://www.example.com')
        'example.com'
    """
    url = str(url).strip().lower()
    if not url:
        return ""
    if not url.startswith(("http://", "https://")):
        url = "https://" + url
    try:
        parsed = urlparse(url)
        domain = parsed.netloc
    except Exception:
        return ""
    parts = [p for p in domain.split(".") if p]
    if len(parts) >= 2:
        return ".".join(parts[-2:])
    return domain


def find_existing_x_directory(dir_roots, dir_base):
    """
    è¿”å›ç¬¬ä¸€ä¸ªå­˜åœ¨çš„ç›®å½•
    """

    for root in dir_roots:
        path = f"{root }/{dir_base}"
        if os.path.exists(path):
            return path
    return None


# æ—¥å¿—æ¶ˆæ¯é˜Ÿåˆ—
log_queue = queue.Queue()
# åˆ†ç±»ç¼“å­˜é”categories_cache_lock
cat_lock = threading.Lock()
# ä½¿ç”¨æ—¶é—´ä¼šå½±å“æ—¥å¿—è¾“å‡ºçš„æ€§èƒ½(æ¯”æ²¡æœ‰æ—¶é—´æˆ–è€…ç›´æ¥printè¦æ…¢å¾—å¤š,åœ¨å¤§é‡æ‰“å°çš„æƒ…å†µä¸‹ä¼šæ‹–ç´¯é€Ÿåº¦)

LOG_HEADER = ["SKU", "Name", "id", "Status", "message", "datetime"]


# ç»Ÿè®¡å¤„ç†åçš„å„ä¸ªcsvæ–‡ä»¶åˆ†åˆ«æœ‰å¤šå°‘æ¡æ•°æ®
def count_lines_csv(csv_dir):
    """ç»Ÿè®¡csvçš„æ•°æ®è¡Œæ•°"""
    total = 0
    for file in os.listdir(csv_dir):
        if not file.endswith(".csv"):
            continue
        file = os.path.join(csv_dir, file)
        df = pd.read_csv(file)
        total += len(df)
        print(file, len(df))

    return total


def get_data_from_csv(args, lines, reader, url_field, name_field):
    """
    ä»csv readerå¯¹è±¡ä¸­è¯»å–å›¾ç‰‡åå­—å’Œå›¾ç‰‡é“¾æ¥å­—æ®µ,å¹¶å†™å…¥ç»“æœåˆ°linesä¸­

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        lines: å›¾ç‰‡æ•°æ®åˆ—è¡¨,å­˜å‚¨è§£æå‡ºæ¥çš„ç»“æœ(urlæˆ–name+url)
        reader: csvæ–‡ä»¶è¯»å–å™¨
        url_field: å›¾ç‰‡é“¾æ¥æ‰€åœ¨çš„åˆ—å
        name_field: å›¾ç‰‡åå­—æ‰€åœ¨çš„åˆ—å
    """
    for line in reader:
        debug("Processing line: %s", line)
        img_names = line.get(name_field, "")
        img_urls = line.get(url_field, "")
        # å°†å¤„ç†ç»“æœä¿å­˜åˆ°img_nameså’Œimg_urlsä¸­
        get_data_line_name_url_from_csv(
            args=args, lines=lines, img_names=img_names, img_urls=img_urls
        )


def get_data_line_name_url_from_csv(args, lines, img_names, img_urls):
    """
    å°†è¯»å–çš„csvæ–‡ä»¶ä¸­çš„å›¾ç‰‡åå­—å’Œå›¾ç‰‡é“¾æ¥,å¤„ç†å•è¡Œ

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        lines: å›¾ç‰‡æ•°æ®åˆ—è¡¨,å­˜å‚¨è§£æå‡ºæ¥çš„ç»“æœ(name,url)
        img_names: å›¾ç‰‡åå­—
        img_urls: å›¾ç‰‡é“¾æ¥
    """
    # ä¸ºäº†å…¼å®¹æ—§çš„è¡¨æ ¼è§„èŒƒ,è¿™é‡Œè¦è®¡ç®—ä¸€ä¸‹img_urlså­—æ®µå–å€¼
    # img_urls = img_urls or img_names
    line_info = f"[[{img_names}] and [{img_urls}]]"
    if not img_urls:
        if img_names:
            img_urls = img_names
            img_names = ""
        else:
            error(f"img_urls and img_names are both empty, skip this line: {lines}")

    debug(f"Get data: {line_info}")

    if img_urls:
        # img_names = img_names.split(",")
        # img_urls = img_urls.split(",")

        # img_names = COMMON_SEP_REGEXP.split(img_names)
        img_names = URL_SEP_REGEXP.split(img_names)
        img_urls = split_urls(img_urls)
    if args.name_url_pairs:
        for img_name, img_url in zip(img_names, img_urls):
            img_name = img_name.strip()
            img_url = img_url.strip()
            lines.append(
                # {
                #     "img_name": img_name,
                #     "img_url": img_url,
                # }
                (img_name, img_url)
            )
    else:
        info("url only:[%s]", img_urls)
        lines.extend(img_urls)


def split_urls(urls):
    """å°†urlæ„æˆçš„å­—ç¬¦ä¸²è§£ææˆä¸€ä¸ªä¸ªurlå­—ç¬¦ä¸²æ„æˆçš„åˆ—è¡¨

    ç¼–å†™åˆé€‚çš„æ­£åˆ™è¡¨è¾¾å¼æ¥æå–url
    1. åŒ¹é…ä»¥http://æˆ–https://å¼€å¤´çš„URL
    2. è€ƒè™‘urlé—´çš„åˆ†éš”ä¸²,å¦‚',','>',ç©ºç™½å­—ç¬¦ç­‰
    :param urls: è¦è¢«è§£æçš„URLå­—ç¬¦ä¸²
    ä¾‹å¦‚è¾“å…¥
    :return: è§£æåçš„URLåˆ—è¡¨

    å¤šä¸ªurlæ„æˆçš„é•¿ä¸²å¤„ç†ä¾‹å­

    """

    if(urls):
        matches = re.findall(HTTP_S_URL_CONS_PATTERN, urls)
    else:
        print("ERROR!!!,urls is invalid")
        matches=[]
    return matches

    # matches = re.findall(HTTP_S_URL_CONS_PATTERN, urls)
    # return matches


def parse_dbs_from_str(dbs_str):
    """è§£ææ•°æ®åº“æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²,è¿”å›ä¸€ä¸ªåˆ—è¡¨

    :param dbs_str: æ•°æ®åº“æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²(ä¸€èˆ¬å»ºè®®ä½¿ç”¨r-string),æ”¯æŒé€—å·åˆ†éš”,æ¢è¡Œåˆ†éš”,åˆ†å·åˆ†éš”
    ä¾‹å¦‚è¾“å…¥
    :return: æ•°æ®åº“æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    è¿”å›å†…å®¹ç¤ºä¾‹
    ['C:\\ç«è½¦é‡‡é›†å™¨V10.27\\Data\\a\\SpiderResult.db3',
    'C:\\ç«è½¦é‡‡é›†å™¨V10.27\\Data\\b\\SpiderResult.db3',
    'C:\\ç«è½¦é‡‡é›†å™¨V10.27\\Data\\c\\SpiderResult.db3',
    '...',
    'C:\\ç«è½¦é‡‡é›†å™¨V10.27\\Data\\z\\SpiderResult.db3']
    """
    dbs = dbs_str.replace("\n", ",").replace(";", ",").split(",")
    dbs = [db for db in dbs if db.strip() != ""]
    return dbs


def remove_sensitive_info(text, try_remove_phone=False):
    """ç§»é™¤æ–‡æœ¬ä¸­çš„æ•æ„Ÿä¿¡æ¯ï¼Œå¦‚é‚®ç®±åœ°å€ã€ç½‘å€
    (ç”µè¯å·ç å®¹æ˜“è¯¯ä¼¤,é»˜è®¤ä¸è¿‡æ»¤)

    å¯ä»¥ä½¿ç”¨regex101 (for python)è¿›è¡Œåœ¨çº¿æµ‹è¯•

    1. åœ¨HTTPå’ŒHTTPS URLä¸­ï¼Œå…è®¸å‡ºç°çš„å­—ç¬¦åŒ…æ‹¬ä½†ä¸é™äºå­—æ¯ï¼ˆa-z, A-Zï¼‰ã€æ•°å­—ï¼ˆ0-9ï¼‰ã€ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚-, _, ., ~ï¼‰ã€ä»¥åŠURLç¼–ç åçš„ä¸€äº›å­—ç¬¦ï¼ˆå¦‚%20è¡¨ç¤ºç©ºæ ¼ï¼‰ã€‚
    ä¸ºäº†ä¿å®ˆåœ°è¿‡æ»¤æ‰ä¸€æ®µæ–‡æœ¬ä¸­çš„HTTPSé“¾æ¥ï¼Œå¯ä»¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…URLçš„ç»“æ„ï¼Œä½†å°½é‡å‡å°‘è¯¯ä¼¤å…¶ä»–æ–‡æœ¬ã€‚


    """
    # ç§»é™¤é‚®ç®±åœ°å€
    text = re.sub(EMAIL_PATTERN, "", text)

    # [ä¿å®ˆ]åœ°ç§»é™¤HTTP(S)é“¾æ¥
    text = re.sub(HTTP_S_URL_CONS_PATTERN, "", text)

    # ç§»é™¤çº¯åŸŸå
    text = re.sub(r"\b(?:www\.)?[a-zA-Z0-9-]+\.[a-zA-Z]{2,}(?:/[^\s]*)?", "", text)

    # ç§»é™¤æ¬§ç¾ç”µè¯å·ç (å®¹æ˜“è¯¯æ€,ä¸å»ºè®®ç”¨,ç”µè¯å·ç ç•™ç€é—®é¢˜ä¹Ÿä¸å¤§)
    if try_remove_phone:
        text = re.sub(
            r"\b(?:\+?\d{1,3}[-.\s]?)?(?:\(?\d{2,4}\)?[-.\s]?)?\d{3,4}[-.\s]?\d{4}\b",
            "",
            text,
        )

    return text


def check_iterable(it):
    """é€è¡Œæ‰“å°å¯è¿­ä»£å¯¹è±¡
    æ£€æŸ¥å‚æ•°itæ˜¯å¦ä¸ºå¯è¿­ä»£å¯¹è±¡,å¦‚æœæ˜¯,åˆ™é€è¡Œæ‰“å°æ¯ä¸ªå…ƒç´ 
    å¦åˆ™,æ‰“å°itæœ¬èº«,å¹¶æŠ¥å‘Šå…¶ç±»å‹æ˜¯ä¸å¯è¿­ä»£çš„
    ä¾‹å¦‚ä¸€è¡Œä¸€ä¸ªå­—å…¸
    """
    if hasattr(it, "__iter__"):
        print(f"total {len(it)} items.")
        if len(it):
            for i, item in enumerate(it, 1):
                # å°†è¿™äº›ä¸è§„èŒƒçš„äº§å“å¸¦åºå·åœ°åˆ—å‡ºæ¥
                print(f"row {i}: {item}")
    else:
        error(f"[{it}] is not iterable.(or empty result)")


def download_img_to_local(img_url, product_sku):
    """
    ä¸‹è½½è¿œç¨‹å›¾ç‰‡å¹¶ä¿å­˜åˆ°æœ¬åœ°
    :param url: å›¾ç‰‡çš„URL
    :param product_sku: äº§å“çš„SKUï¼Œç”¨äºå‘½åæœ¬åœ°å›¾ç‰‡
    :return: æœ¬åœ°å›¾ç‰‡è·¯å¾„
    """
    local_filename = f"{product_sku}.jpg"
    with requests.get(img_url, stream=True, timeout=10) as r:
        r.raise_for_status()
        with open(local_filename, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
    return local_filename


def split_list(lst, n):
    """å°†å¯è¿­ä»£å®¹å™¨(ä¾‹å¦‚åˆ—è¡¨)å¹³å‡åˆ†ä¸ºnä»½,è¿”å›åˆ†å‰²åçš„åˆ—è¡¨
    å°½å¯èƒ½çš„å¹³å‡åˆ†é…æ¯”è¾ƒå¥½,å¦‚æœå‰n-1ä»½æ•°é‡ç›¸åŒ,ä¸”
    å°†ä½™æ•°å…ƒç´ éƒ½æ”¾åˆ°æœ€åä¸€ä»½å¯èƒ½ä¼šå¯¼è‡´æœ€åä¸€ä»½å…ƒç´ è¿‡å¤š(æœ€åæƒ…å†µä¸‹æ˜¯å¹³å‡ä»½çš„è¿‘ä¸¤å€)
    æ–¹æ¡ˆ1:æ¨è
    è®¾ä½ è¦å‡åˆ†ä¸ºnä»½,æ¯ä»½çš„åŸºæœ¬å¤§å°ä¸ºsize=len(lst)//n,ä½™æ•°ä¸ºr=len(lst)%n,ä¸”ä¸€å®šæœ‰(r<n)
    é‚£ä¹ˆå‰rä»½æ¯ä»½å«æœ‰size+1ä¸ªå…ƒç´ ï¼Œån-rä»½æ¯ä»½å«æœ‰sizeä¸ªå…ƒç´ 
    æ­¤æ–¹æ¡ˆå¯ä»¥ä¿è¯,ä»»æ„ä¸¤ä»½æ‰€åˆ†å¾—çš„å…ƒç´ æ•°é‡å·®ä¸ä¼šè¶…è¿‡1
    å¦‚æœnå¤§äºlstçš„å…ƒç´ æ•°é‡,é‚£ä¹ˆå‰len(lst)ä»½æ¯ä»½å«æœ‰1ä¸ªå…ƒç´ ,ån-len(lst)ä»½æ¯ä»½å«æœ‰0ä¸ªå…ƒç´ (ç©ºåˆ—è¡¨)

    æ–¹æ¡ˆ2:ä¸æ¨è
    å¦‚æœå®¹å™¨ä¸­å…ƒç´ é•¿åº¦ä¸ºL=len(lst),kä¸ºL/nçš„ä½™æ•°,åˆ™å°†æœ€åkä¸ªå…ƒç´ å½’å…¥æœ€åä¸€ä»½

    Parameters
    ----------
    lst : list
        éœ€è¦è¢«åˆ†å‰²çš„å®¹å™¨åˆ—è¡¨
    n : int
        éœ€è¦è¢«åˆ†å‰²çš„ä»½æ•°(å¹³å‡)

    Returns
    -------
    list
        åˆ†å‰²åçš„åˆ—è¡¨çš„åˆ—è¡¨
    Examples
    --------
    split_list(list(range(1,11)), 4)


    """
    result = []
    size, r = divmod(len(lst), n)
    info(f"size: {size}, r: {r}")

    size_r = size + 1
    for i in range(r):
        start = i * size_r
        end = (i + 1) * size_r
        result.append(lst[start:end])
    start = (size + 1) * r

    for i in range(r, n):
        end = start + size
        result.append(lst[start:end])
        start = end

    return result


def log_worker(log_file="./"):
    """åå°æ—¥å¿—è®°å½•çº¿ç¨‹
    åˆ©ç”¨å¾ªç¯ä¸æ–­å°è¯•ä»å…¨å±€æ—¥å¿—é˜Ÿåˆ—ä¸­è·å–æ—¥å¿—æ¡ç›®,ç„¶åå†™å…¥åˆ°æ—¥å¿—æ–‡ä»¶ä¸­
    log_header å‚è€ƒ:["Timestamp", "RecordID", "Status", "Details", "ProcessingTime"]

    """
    info(f"Log worker started.logs will be written to: {log_file}ğŸˆ")
    while True:
        log_entry = log_queue.get()
        debug(f"Log worker got log entry: {log_entry}")
        if log_entry is None:  # ç»ˆæ­¢ä¿¡å·
            break
        try:
            # æ£€æŸ¥è·¯å¾„(ç›®å½•)å­˜åœ¨,è‹¥ä¸å­˜åœ¨åˆ™åˆ›å»º
            log_dir = os.path.dirname(log_file)
            if not os.path.exists(log_dir):
                os.makedirs(log_dir)
            # å†™å…¥æ—¥å¿—æ–‡ä»¶
            with open(log_file, "a", newline="", encoding="utf-8") as f:

                writer = csv.writer(f)
                # å¦‚æœæ–‡ä»¶ä¸ºç©ºï¼Œå†™å…¥æ ‡é¢˜è¡Œ
                if f.tell() == 0:
                    writer.writerow(LOG_HEADER)
                writer.writerow(log_entry)
        except Exception as e:
            error(f"Error:Log write failed: {e}")
        finally:
            log_queue.task_done()


def log_upload(sku, name, product_id, status, msg=""):
    """å°†æ—¥å¿—åŠ å…¥åˆ°æ—¥å¿—æ¶ˆæ¯é˜Ÿåˆ—ä¸­
    è¡¨å¤´ç»“æ„ç”±å¸¸é‡LOG_HEADERå®šä¹‰,å…ƒç´ é¡ºåºä¸LOG_HEADERä¸€è‡´,æˆ–è€…ä½¿ç”¨å…³é”®å­—å‚æ•°ä¼ å‚
    """
    log_entry = [
        sku,
        name,
        product_id,
        status,
        msg,
        datetime.now().isoformat(),
    ]
    log_queue.put(log_entry)
    debug(f"Log preview:{log_entry}")
    return log_entry


def cleanup_log_thread(q_thread):
    """æ¸…ç†å‡½æ•°ï¼Œç­‰å¾…é˜Ÿåˆ—å¤„ç†å®Œæˆå¹¶åœæ­¢å·¥ä½œçº¿ç¨‹"""
    log_queue.join()  # ç­‰å¾…æ‰€æœ‰æ—¥å¿—é¡¹å¤„ç†å®Œæˆ
    log_queue.put(None)  # å‘é€ç»ˆæ­¢ä¿¡å·
    q_thread.join()  # ç­‰å¾…çº¿ç¨‹ç»“æŸ
    info("log_thread(daemon) end.")


##
