"""å›¾ç‰‡ä¸‹è½½å™¨"""

import argparse
import csv
import logging
import os
import re
import sys

# from logging import error, exception, info, warning, debug
from comutils import (
    URL_SEP_PATTERN,
    URL_SEPARATORS,
    get_user_choice_csv_fields,
    get_data_from_csv,
    # split_multi,
)
from imgdown import ImageDownloader, USER_AGENTS
from filenamehandler import FilenameHandler as fh
from wooenums import CSVProductFields

RESIZE_THRESHOLD = (1000, 800)
DEAFULT_EXT = ".webp"
csv.field_size_limit(int(1e7))  # å…è®¸csvæ–‡ä»¶æœ€å¤§ä¸º10MB
# æˆ–è€…æ ¹æ®å®é™…ç±»å®šä¹‰ä½ç½®è°ƒæ•´å¯¼å…¥è·¯å¾„
IMG_DIR = "./images"
selected_csv_field_ids: list[str] = []
# æ—¥å¿—é…ç½®
logger = logging.getLogger("ImageDownloader")
info = logger.info
debug = logger.debug
warning = logger.warning
error = logger.error
exception = logger.exception

if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_formatter = logging.Formatter(
        fmt="%(asctime)s [%(levelname)s] %(funcName)s: %(message)s"
    )
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
# é»˜è®¤ INFOï¼Œmain() é‡Œæ ¹æ® -v å†è°ƒæ•´
logging.basicConfig(level=logging.INFO)
debug("Logger initialized %s", logging.getLevelName(logger.level))


def parse_image_sources(file, args, lines, selected_ids=None):
    """
    è§£æè¾“å…¥æ–‡ä»¶ï¼Œæå–å›¾ç‰‡ä¸‹è½½æ‰€éœ€çš„URLå’Œæ–‡ä»¶åã€‚

    å¯¹äºå¤„ç†å¤šä¸ªæ–‡ä»¶,ä¸”å¤šä¸ªæ–‡ä»¶çš„æ ¼å¼ä¸€æ ·(ä¸»è¦é’ˆå¯¹csvæ–‡ä»¶),å¯ä»¥é€‰æ‹©è®°ä½å‚æ•°,é¿å…åç»­åå¤è¦æ±‚å¡«å†™åˆ—å·
    (å½“ç„¶ä¹Ÿå¯ä»¥DFç‰¹å®šçš„csvæ–‡ä»¶æ ¼å¼ä¹Ÿå¯ä»¥å½“åšæ™®é€šcsvçœ‹å¾…,ä½†æ˜¯åè€…æœ‰ä¸“é—¨çš„å‚æ•°ä¼šæ›´åŠ ç›´æ¥)

    :param file_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
    :param args: å‘½ä»¤è¡Œå‚æ•°å¯¹è±¡ï¼ˆåŒ…å« from_specific_csv, from_csv ç­‰æ ‡å¿—ï¼‰
    :param lines: ç”¨äºå­˜å‚¨ç»“æœçš„åˆ—è¡¨(å¤šæ¬¡è°ƒç”¨å°†ä¼šç´¯åŠ åˆ°è¿™ä¸ªåˆ—è¡¨ä¸­)ï¼Œæ ¼å¼ä¸º [(name, url), ...] æˆ– [url, ...]
    :return: æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥æŠ›å‡ºå¼‚å¸¸æˆ–è¿”å› False
    """

    try:
        with open(file=file, mode="r", encoding="utf-8") as f:
            if args.from_specific_csv:
                # DFå›¢é˜Ÿç‰¹å®šcsvæ–‡ä»¶æ ¼å¼
                csv_dict_reader = csv.DictReader(f)
                name_field = CSVProductFields.IMAGES.value
                url_field = CSVProductFields.IMAGES_URL.value

                get_data_from_csv(args, lines, csv_dict_reader, url_field, name_field)

            elif args.from_csv:
                # é’ˆå¯¹ä¸€èˆ¬çš„å«æœ‰å›¾ç‰‡é“¾æ¥çš„csvæ–‡ä»¶,æ›´åŠ çµæ´»(ä¹Ÿèƒ½å¤Ÿå¤„ç†ä¸Šé¢çš„æƒ…å†µ,ä½†æ˜¯ä¸Šé¢ä¸“ç”¨åˆ†æ”¯ä¼šæ›´åŠ å¿«æ·)
                csv_dict_reader = csv.DictReader(f)
                reader_headers = csv_dict_reader.fieldnames or []
                fmt_consistent = args.format_consistent
                debug("Use selected_ids: %s", selected_ids)
                debug("Use fmt_consistent: %s", fmt_consistent)
                if fmt_consistent and selected_ids:
                    # æ ¼å¼ä¸€è‡´ä¸”æŒ‡å®šäº†åˆ—å·,ç›´æ¥ä½¿ç”¨è®°ä½çš„å‚æ•°(ä¸éœ€è¦æ¯ä¸ªæ–‡ä»¶éƒ½è¯¢é—®)
                    pass
                else:
                    # æ‰“å°å‡ºcsvæ–‡ä»¶ä¸­æ‰€æœ‰å­—æ®µå,è®©ç”¨æˆ·é€‰æ‹©ğŸˆ
                    selected_ids = get_user_choice_csv_fields(
                        selected_ids, reader_headers
                    )

                selected_ids = selected_ids or []
                if len(selected_ids) == 2:
                    # åˆ†åˆ«è§£æå­—æ®µç´¢å¼•,ç„¶åè·å–å­—æ®µå
                    name_field_id, url_field_id = selected_ids
                    name_field = reader_headers[int(name_field_id)]
                    url_field = reader_headers[int(url_field_id)]
                elif len(selected_ids) == 1:
                    url_field = reader_headers[int(selected_ids[0])]
                    name_field = ""
                else:
                    raise ValueError("è¯·æŒ‰ç…§æ­£ç¡®çš„æ ¼å¼è¾“å…¥åˆ—å·")

                get_data_from_csv(args, lines, csv_dict_reader, url_field, name_field)

            else:
                for line in f:
                    parts = re.split(pattern=URL_SEP_PATTERN, string=line.strip())
                    if args.name_url_pairs:
                        if len(parts) == 2:
                            name, url = parts
                            lines.append((name.strip(), url.strip()))
                        else:
                            warning("æ— æ•ˆçš„è¡Œæ ¼å¼: %s; parts=%s", line, parts)
                    else:
                        lines.append(line.strip())

                if not lines:
                    error("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶åå’ŒURLå¯¹")
                    return False

        return True

    except Exception as e:
        error("è¯»å–æ–‡ä»¶å¤±è´¥: %s", str(e))
        return False


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°

    åˆ©ç”¨parser.add_argument()æ–¹æ³•æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶è§£æå‘½ä»¤è¡Œå‚æ•°
    é•¿é€‰é¡¹--å¼€å¤´,ä¾‹å¦‚--workers,æŒ‡å‡ºå‚æ•°å°†ä¼šç»‘å®šåˆ°ç›¸åº”çš„å˜é‡ä¸Š,ç»è¿‡parse_args()è§£æ,å°†æ„é€ å¯¹åº”çš„å‚æ•°åŒ…
    """
    parser = argparse.ArgumentParser(description="å¤šçº¿ç¨‹å›¾ç‰‡ä¸‹è½½å™¨")
    # å…³äºè¾“å…¥çš„å‚æ•°(å›¾ç‰‡é“¾æ¥)çš„è‹¥å¹²å½¢å¼
    parser.add_argument(
        "-i",
        "--test-url",
        help="æ‰‹åŠ¨æŒ‡å®šè‹¥å¹²å›¾ç‰‡URLè¿›è¡Œæµ‹è¯•ä¸‹è½½,å¦‚æœæœ‰å¤šä¸ªURL,ç”¨ç©ºæ ¼åˆ†éš”",
    )
    parser.add_argument(
        "-f",
        "--file-input",
        nargs="+",
        required=False,
        help="åŒ…å«å›¾ç‰‡URLçš„è¾“å…¥[æ–‡ä»¶],å…è®¸æŒ‡å®šå¤šä¸ªæ–‡ä»¶",
    )
    parser.add_argument(
        "-d",
        "--dir-input",
        nargs="+",
        required=False,
        help="åŒ…å«å›¾ç‰‡URLçš„æ–‡ä»¶æ‰€åœ¨[ç›®å½•(æ–‡ä»¶å¤¹)]ï¼Œå…è®¸æŒ‡å®šå¤šä¸ªç›®å½•(todo)",
    )
    # å…³äºè¾“å…¥çš„é¢å¤–å±æ€§æè¿°ç›¸å…³å‚æ•°
    parser.add_argument(
        "-a",
        "--format-consistent",
        action="store_true",
        help="æ‰€æœ‰è¾“å…¥æ–‡ä»¶æ˜¯å¦ä¸ºç›¸åŒçš„æ ¼å¼(å¦‚æœæ˜¯,å¯ä»¥é¿å…å¤šæ¬¡è¯¢é—®æ–‡ä»¶æ ¼å¼)",
    )
    parser.add_argument(
        "-c",
        "--from-specific-csv",
        action="store_true",  # å¼€å…³å¼å‚æ•°
        help="æŒ‡å®šçš„æ–‡ä»¶æ˜¯ç‰¹å®šcsvæ–‡ä»¶(dfå›¢é˜Ÿå®šåˆ¶æ ¼å¼),ä»csvæ–‡ä»¶ä¸­æŒ‡å®šçš„å›¾ç‰‡åå­—/é“¾æ¥ä¸‹è½½",
    )
    parser.add_argument(
        "-C",
        "--from-csv",
        action="store_true",  # å¼€å…³å¼å‚æ•°
        help="æŒ‡å®šçš„æ–‡ä»¶æ˜¯æ™®é€šcsvæ–‡ä»¶,ä»csvæ–‡ä»¶ä¸­ä¸‹è½½æ‰€æœ‰å›¾ç‰‡",
    )
    parser.add_argument(
        "-n",
        "--name-url-pairs",
        action="store_true",  # å¼€å…³å¼å‚æ•°
        help=f'è¾“å…¥æ–‡ä»¶åŒ…å«æ–‡ä»¶åå’ŒURLå¯¹ï¼Œæ ¼å¼ä¸º"æ–‡ä»¶å URL"ï¼Œä»¥[{URL_SEPARATORS}]ä¸­æŒ‡å®šçš„ç¬¦å·åˆ†éš”',
    )
    # è¾“å‡ºå‚æ•°å’Œæ§åˆ¶
    parser.add_argument(
        "-o",
        "--output-dir",
        default=IMG_DIR,
        help="å›¾ç‰‡ä¿å­˜ç›®å½• (é»˜è®¤: ./images)"
        "æ­¤ä¸‹è½½å™¨è®¾è®¡ä¸ºæ‰¹é‡ä¸‹è½½,å¦‚æœè¦æŒ‡å®šæ–‡ä»¶çš„ä¿å­˜åå­—,éœ€è¦åœ¨æ‰¹é‡è¾“å…¥(æ¯”å¦‚è¡¨æ ¼æ–‡ä»¶)ä¸­æŒ‡å®šæ¯ä¸ªå›¾ç‰‡çš„ä¿å­˜å"
        "å¯¹äºå•ä¸ªä¸‹è½½å›¾ç‰‡é“¾æ¥çš„æµ‹è¯•è¡Œä¸º,æ­¤é€‰é¡¹åº”è¯¥å°†ç†è§£ä¸ºè¾“å‡ºç›®å½•è€Œä¸æ˜¯å•ä¸ªè¾“å‡ºæ–‡ä»¶è·¯å¾„"
        "å¯ä»¥è€ƒè™‘å¢åŠ -opé€‰é¡¹,ç”¨æ¥é’ˆå¯¹ä¸‹è½½å•ä¸ªå›¾ç‰‡é“¾æ¥æ—¶æŒ‡å®šä¿å­˜æ–‡ä»¶å(todo)",
    )
    parser.add_argument(
        "-O", "--override", action="store_true", default=False, help="æ˜¯å¦è¦†ç›–å·²æœ‰å›¾ç‰‡"
    )
    # ä¸‹è½½æ–¹æ¡ˆæ§åˆ¶
    parser.add_argument(
        "-U",
        "--use-shutil",
        default="",
        choices=["request", "curl", "iwr", "playwright"],
        # action="store_true",
        help="ä½¿ç”¨python è¯·æ±‚æˆ–å¤–éƒ¨å·¥å…·ä¸‹è½½å›¾ç‰‡(request,curl,iwr,playwright)",
    )
    parser.add_argument("-w", "--workers", type=int, default=10, help="ä¸‹è½½çº¿ç¨‹æ•°")
    parser.add_argument(
        "-t", "--timeout", type=int, default=30, help="ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼Œå•ä½ç§’"
    )
    parser.add_argument("-r", "--retry", type=int, default=0, help="ä¸‹è½½å¤±è´¥é‡è¯•æ¬¡æ•° ")
    parser.add_argument("-R", "--quality-rule", help="å‹ç¼©å›¾ç‰‡çš„è´¨é‡è§„åˆ™")
    parser.add_argument(
        "-u", "--user-agent", default=USER_AGENTS[0], help="è‡ªå®šä¹‰User-Agent"
    )
    parser.add_argument(
        "-F",
        "--fake-format",
        action="store_true",
        help="åœ¨ä½“ç§¯æ²¡æœ‰ç¼©å°çš„æƒ…å†µä¸‹,å°†åŸå›¾ç‰‡çš„åç¼€æ›´æ”¹ä¸ºæŒ‡å®šçš„è¾“å‡ºæ ¼å¼ç›¸åŒ",
    )
    parser.add_argument(
        "-ps",
        "--ps-version",
        default="powershell",
        choices=["powershell", "pwsh"],
        help="PowerShellç‰ˆæœ¬ï¼Œå¯é€‰å€¼ï¼špowershellã€pwsh",
    )
    parser.add_argument(
        "-ci",
        "--curl-insecure",
        action="store_true",
        help="å¿½ç•¥curlè¯ä¹¦éªŒè¯å’Œæ£€æŸ¥(ä¸ºcurlå¯ç”¨-k,--ssl-no-revoke)",
    )
    parser.add_argument(
        "-s",
        "--verify-ssl",
        action="store_true",
        help="æ˜¯å¦éªŒè¯SSLè¯ä¹¦(å¯ç”¨ä¼šæé«˜å®‰å…¨æ€§ï¼Œä½†å¯èƒ½é™ä½ä¸‹è½½é€Ÿåº¦ä»¥åŠæˆåŠŸç‡)",
    )
    parser.add_argument("--proxy-file", help="ä»£ç†IPåœ°å€åˆ—è¡¨æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--cookie-file", help="åŒ…å«Cookiesçš„JSONæ–‡ä»¶è·¯å¾„")

    parser.add_argument("-v", "--verbose", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—")
    parser.add_argument(
        "-x",
        "--compress-quality",
        type=int,
        default=0,
        help="å‹ç¼©å›¾ç‰‡ä¸ºwebpæ ¼å¼çš„qualityå‚æ•°(1-100),å–0è¡¨ç¤ºä¸å‹ç¼©",
    )
    parser.add_argument(
        "-k",
        "--remove-original",
        action="store_true",
        help="ä¿ç•™å‹ç¼©åçš„åŸå§‹å›¾ç‰‡",
    )
    parser.add_argument(
        "-rs",
        "--resize-threshold",
        type=int,
        nargs=2,
        # default=RESIZE_THRESHOLD,
        help="æŒ‡å®šå›¾ç‰‡ç­‰æ¯”ä¾‹ç¼©æ”¾åçš„æœ€å¤§å°ºå¯¸(å®½,é«˜),å•ä½px;æ”¾ç©ºè¡¨ç¤ºä¸è°ƒæ•´åˆ†è¾¨ç‡",
    )

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""

    # è§£æå‘½ä»¤è¡Œç”¨æˆ·ä¼ è¾“è¿›æ¥çš„å‚æ•°,åƒå­—å…¸ä¸€æ ·ä½¿ç”¨å®ƒ
    args = parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    # æ‰“å°å½“å‰çš„æ—¥å¿—çº§åˆ«:
    print(f"å½“å‰æ—¥å¿—çº§åˆ«: {logging.getLevelName(logger.level)}")
    debug("å½“å‰æ—¥å¿—çº§åˆ«: %s", logging.getLevelName(logger.level))
    # è¯»å–è¾“å…¥æ–‡ä»¶
    lines = []
    if args.test_url:
        # å¤„ç†æµ‹è¯•URLä¸‹è½½
        lines = re.split(URL_SEP_PATTERN, args.test_url)
    elif args.file_input:
        # å¤„ç†æ–‡ä»¶è¾“å…¥
        files = args.file_input
        # files = split_multi(files)
        for file in files:
            # è§£ææ‰€æœ‰éœ€è¦è¢«å¤„ç†çš„æ–‡ä»¶,å°†ç»“æœä¿å­˜åœ¨lineså˜é‡ä¸­
            parse_image_sources(
                file=file, args=args, lines=lines, selected_ids=selected_csv_field_ids
            )

        if lines:
            print(f"è¯»å–è¡Œæ•°: {len(lines)}")

        else:
            error("è¯»å–è¡Œæ•°ä¸º0,è¯·æ£€æŸ¥å‚æ•°")
            exit(1)
    elif args.dir_input:
        # å¤„ç†ç›®å½•è¾“å…¥(éå†ç›®å½•ä¸‹çš„æ–‡ä»¶,è½¬æ¢åˆ°æ–‡ä»¶å¤„ç†çš„æƒ…å†µ)ğŸˆ
        dirs = args.dir_input
        # dirs = split_multi(dirs)
        if not dirs:
            error("è¯·æŒ‡å®šç›®å½•!")
            exit(1)
        else:
            for d in dirs:
                if not os.path.exists(d):
                    error("æŒ‡å®šçš„ç›®å½•ä¸å­˜åœ¨: %s", dirs)
                    sys.exit(1)
                else:
                    print(f"å¤„ç†ç›®å½•: [{d}]")
                for file in os.listdir(d):
                    info("å¤„ç†æ–‡ä»¶: %s", file)
                    _, ext = os.path.splitext(file)
                    if ext not in [".csv", ".txt"]:
                        debug("å¿½ç•¥écsvæˆ–txtæ–‡ä»¶: %s", file)
                        continue
                    file = os.path.abspath(os.path.join(d, file))
                    # è§£ææ‰€æœ‰éœ€è¦è¢«å¤„ç†æ–‡ä»¶,å°†ç»“æœä¿å­˜åœ¨lineså˜é‡ä¸­ğŸˆ
                    parse_image_sources(
                        file=file,
                        args=args,
                        lines=lines,
                        selected_ids=selected_csv_field_ids,
                    )
                    # print(lines,"ğŸˆğŸˆ")
    debug(f"use shutil:{args.use_shutil}")
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = ImageDownloader(
        max_workers=args.workers,
        timeout=args.timeout,
        retry_times=args.retry,
        user_agent=args.user_agent,
        use_shutil=args.use_shutil,  # deprecated
        download_method=args.use_shutil,
        compress_quality=args.compress_quality,
        quality_rule=args.quality_rule,
        remove_original=args.remove_original,
        override=args.override,
        resize_threshold=args.resize_threshold,
        ps_version=args.ps_version,
        curl_insecure=args.curl_insecure,
        fake_format=args.fake_format,
    )
    # è¿‡æ»¤å·²æœ‰å›¾ç‰‡,æ‰«æå‡ºå°šæœªä¸‹è½½çš„å›¾ç‰‡
    # è¿™é‡Œä¸å…³å¿ƒæ–‡ä»¶ååç¼€çš„å·®å¼‚,æ¯”è¾ƒbasename
    ## è¯»å–æŒ‡å®šç›®å½•ä¸‹çš„å›¾ç‰‡(åªåˆ—å‡ºåå­—)
    if not os.path.exists(args.output_dir):
        warning("æŒ‡å®šçš„è¾“å‡ºç›®å½•[%s]ä¸å­˜åœ¨(å°†å°è¯•è‡ªåŠ¨åˆ›å»º)", args.output_dir)
    elif not args.override:
        # æŸ¥è¯¢æŒ‡å®šç›®å½•ä¸‹çš„å·²æœ‰å›¾ç‰‡ä»¥åŠå»é‡å¤„ç†
        # å¦‚æœæŒ‡å®šçš„å­˜æ”¾ç›®å½•å­˜åœ¨
        img_names_existed = os.listdir(args.output_dir)
        # é»˜è®¤æƒ…å†µä¸‹,å¯¹æ¯”é‡å¤ä¸‹è½½æ—¶,æˆ‘ä»¬åªå…³å¿ƒæ–‡ä»¶å,ä¸å…³å¿ƒåç¼€
        img_names_existed = [os.path.splitext(name)[0] for name in img_names_existed]
        # è®°å½•è¿‡æ»¤å‰çš„å¾…ä¸‹è½½å›¾ç‰‡æ•°é‡
        total_num_raw = len(lines)
        if args.name_url_pairs:
            # ä»äºŒå…ƒç»„ä¸­è§£æå‡ºåå­—
            lines = [
                (name, _)
                for name, _ in lines
                if fh.get_filebasename_from_url_or_path(name)
                not in img_names_existed  # è¿™é‡Œè¿›è¡ŒæŸ¥é‡,ä»…æ¯”è¾ƒå›¾ç‰‡åå­—(ä¸åŒ…æ‹¬åç¼€,ä½¿ç”¨å¯¹åº”çš„å‡½æ•°æˆªå–å›¾ç‰‡åŸºå)
            ]
            # print(lines,"ğŸˆğŸˆ")
            # return
        else:
            # ä»URLåˆ—è¡¨ä¸­è§£æå‡ºåå­—
            lines = [
                url
                for url in lines
                if fh.get_filebasename_from_url_or_path(url) not in img_names_existed
            ]
        total_num_filtered = len(lines)
        # ç»Ÿè®¡å¤šå°‘å›¾ç‰‡è¢«è¿‡æ»¤æ‰
        num_filtered = total_num_raw - total_num_filtered
        info(
            "è¿‡æ»¤æ‰%då¼ å›¾ç‰‡(è¿‡æ»¤å‰ååˆ†åˆ«æœ‰: %då¼ , %då¼ )",
            num_filtered,
            total_num_raw,
            total_num_filtered,
        )

    # ä¸‹è½½å›¾ç‰‡
    if args.name_url_pairs:
        # è§£ææ–‡ä»¶åå’ŒURLå¯¹(ä½¿ç”¨è‡ªå®šä¹‰æ–‡ä»¶å)
        try:
            downloader.download_with_names(
                name_url_pairs=lines,
                output_dir=args.output_dir,
                default_ext=DEAFULT_EXT,
            )
        except Exception as e:
            exception("ä¸‹è½½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: %s", str(e))
            return 1
    else:
        # ç›´æ¥ä¸‹è½½URLåˆ—è¡¨ä¸­çš„å›¾ç‰‡
        try:
            if not lines:
                warning("æ²¡æœ‰æœ‰æ•ˆçš„URL")
                return 1

            downloader.download_only_url(
                urls=lines, output_dir=args.output_dir, default_ext=DEAFULT_EXT
            )
        except Exception as e:
            exception("ä¸‹è½½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: %s", str(e))
            return 1

    return 0


if __name__ == "__main__":
    info("welcome to use image downloader!")
    sys.exit(main())
