"""
@last update:20251114
ç®€å•ä½¿ç”¨ç¤ºä¾‹:(æ›´å¤šç»†èŠ‚æŸ¥çœ‹ç›¸å…³æ–‡æ¡£)
æ¨èä½¿ç”¨pwshä½œä¸ºå‘½ä»¤è¡Œç¯å¢ƒ(é¢„è®¾$localhostä¸ºå½“å‰æ¡Œé¢ä¸Šçš„localhostç›®å½•)
ls *.txt|%{python $localhost/get_html.py $_ -o htmls -p $localhost/proxies_nolimit.conf -c 2 -r 1 -t 100 -d 1-3 }

"""

import argparse
import asyncio
import os
import random
import logging
import json
import sys
import re
from urllib.parse import urlparse
from datetime import datetime
from playwright.async_api import async_playwright
import unicodedata
import time

# å¼ºåˆ¶è®¾ç½®Pythonè¾“å‡ºç¼–ç ä¸ºutf-8ï¼Œé˜²æ­¢Windowsç»ˆç«¯ä¹±ç 
os.environ["PYTHONIOENCODING"] = "utf-8"


class UnicodeSafeStreamHandler(logging.StreamHandler):
    """å¤„ç†æ§åˆ¶å°è¾“å‡ºçš„Unicodeç¼–ç é—®é¢˜"""

    def emit(self, record):
        try:
            msg = self.format(record)
            stream = self.stream
            # å¼ºåˆ¶ç”¨utf-8ç¼–ç è¾“å‡ºï¼Œé˜²æ­¢Windowsç»ˆç«¯ä¹±ç 
            msg = msg.encode("utf-8", errors="replace").decode("utf-8")
            stream.write(msg + self.terminator)
            self.flush()
        except Exception:
            self.handleError(record)


class WebSourceDownloader:
    """ç½‘ç»œèµ„æºä¸‹è½½å™¨
    å…¸å‹èµ„æºä¸ºhtml,gz,xmlç­‰
    ä¸»è¦ç”¨äº:(ä¸ä¿è¯ä¸€å®šå¯è¡Œ,ç»“åˆä»£ç†æ± (å¯ä»¥æ˜¯è‡ªå·±ç»´æŠ¤ä¸€ä¸ªå°å‹çš„ä»£ç†æ± )å¯ä»¥æé«˜æˆåŠŸç‡å’Œæ•ˆç‡,å°½ç®¡è¿™ä¸æ˜¯å¿…é¡»çš„)
    1.ä¸‹è½½jsåŠ¨æ€åŠ è½½è¯¦æƒ…é¡µçš„æƒ…å†µ
    2.æ£€æµ‹å®¢æˆ·ç«¯æ˜¯å¦å¯ä»¥æ‰§è¡Œjsçš„ç½‘é¡µ,å¦‚æœæ— æ³•æ‰§è¡Œjså°±ç¦æ­¢è®¿é—®(403),æ¯”å¦‚cloudflareæä¾›çš„è¾ƒé«˜ç­‰çº§çš„é˜²æŠ¤ç½‘ç«™(æ³¨æ„çº¿ç¨‹æ•°æ§åˆ¶ä¸å®œè¿‡é«˜)
    """

    def __init__(
        self,
        output_dir,
        timeout,
        delay_range,
        headless,
        proxy_configs,
        max_concurrency,
        max_retries,
        input_file,
        allow_direct=False,
    ):
        self.output_dir = output_dir
        self.timeout = timeout
        self.delay_range = delay_range
        self.headless = headless
        self.proxy_configs = proxy_configs if proxy_configs else []
        self.allow_direct = allow_direct
        if allow_direct:
            self.proxy_configs.append(None)  # None è¡¨ç¤ºç›´è¿
        self.max_concurrency = max_concurrency
        self.current_concurrency = max_concurrency
        self.max_retries = max_retries
        self.input_file = input_file

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)

        # åˆå§‹åŒ–æ—¥å¿—ç›®å½• (å§‹ç»ˆåœ¨è¾“å‡ºç›®å½•ä¸‹çš„ _logs)
        self.log_dir = os.path.join(self.output_dir, "_logs")
        os.makedirs(self.log_dir, exist_ok=True)

        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self._setup_logging()

        # çŠ¶æ€ç®¡ç†(å®šä¹‰çŠ¶æ€ä¿¡æ¯æ•°æ®æ ¼å¼æ ‡å‡†,å¯å¯¼å‡ºä¸ºjson)
        self.state = {
            "completed": {},  # é”®æ˜¯ URLï¼Œå€¼æ˜¯ True (æˆ–å¤±è´¥ä¿¡æ¯)
            "failed": {},  # é”®æ˜¯ URLï¼Œå€¼æ˜¯ True (æˆ–å¤±è´¥ä¿¡æ¯)
            "total_count": 0,
            "success_count": 0,
            "fail_count": 0,
        }
        self._load_state()

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        self.logger = logging.getLogger("__main__")
        self.logger.setLevel(logging.INFO)

        # æ¸…é™¤å·²æœ‰çš„handlersï¼Œé˜²æ­¢é‡å¤æ—¥å¿—
        if self.logger.hasHandlers():
            self.logger.handlers.clear()

        # æ–‡ä»¶æ—¥å¿—å¤„ç†å™¨(UTF-8ç¼–ç )
        file_handler = logging.FileHandler(
            os.path.join(self.log_dir, "download.log"), encoding="utf-8"
        )
        fmt = "%(asctime)s [%(levelname)s] [%(progress)s] %(message)s"
        file_handler.setFormatter(
            logging.Formatter(
                fmt,
                datefmt="%Y-%m-%d%H:%M:%S",
            )
        )

        # æ§åˆ¶å°æ—¥å¿—å¤„ç†å™¨
        console_handler = UnicodeSafeStreamHandler()
        console_handler.setFormatter(
            logging.Formatter(
                fmt,
                datefmt="%H:%M:%S",
            )
        )

        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)

    def _load_state(self):
        """åŠ è½½ä¸‹è½½çŠ¶æ€ (ç²¾ç®€åŠ è½½)"""
        state_file = os.path.join(self.log_dir, "download_state.json")
        if os.path.exists(state_file):
            try:
                with open(state_file, "r", encoding="utf-8") as f:
                    loaded_state = json.load(f)
                    self.state = loaded_state

                    # # **ä»…åŠ è½½ URL åˆ—è¡¨ï¼Œå€¼ç»Ÿä¸€è®¾ç½®ä¸º True**
                    # self.state["completed"] = {
                    #     url: True for url in loaded_state.get("completed", {}).keys()
                    # }
                    # self.state["failed"] = {
                    #     url: True for url in loaded_state.get("failed", {}).keys()
                    # }

                    # self.state["success_count"] = len(self.state["completed"])
                    # self.state["fail_count"] = len(self.state["failed"])
                self.logger.info(
                    f"æˆåŠŸåŠ è½½ä¸Šæ¬¡è¿›åº¦: å·²å®Œæˆ {self.state['success_count']} ä¸ª, å¤±è´¥ {self.state['fail_count']} ä¸ªã€‚",
                    extra={"progress": "RESUME"},
                )
            except Exception as e:
                self.logger.warning(
                    f"åŠ è½½çŠ¶æ€æ–‡ä»¶å¤±è´¥æˆ–æ–‡ä»¶æ ¼å¼é”™è¯¯: {str(e)}ã€‚å°†ä»é›¶å¼€å§‹ã€‚",
                    extra={"progress": "INIT"},
                )
        else:
            self.logger.info(
                "æœªæ‰¾åˆ°ä¸Šæ¬¡è¿›åº¦æ–‡ä»¶,å°†ä»é›¶å¼€å§‹ã€‚", extra={"progress": "INIT"}
            )
        return self.state

    def _save_state(self):
        """æ›´æ–°/ä¿å­˜ä¸‹è½½çŠ¶æ€ (ç²¾ç®€ä¿å­˜: ä¸»è¦è®°å½• URL é”®)
        è¯»å–å¯¹è±¡ä¸­å­˜å‚¨çš„å…³äºçŠ¶æ€çš„å±æ€§,ç»„ç»‡æˆæ—¢å®šçš„æ ¼å¼ä¿å­˜ä¸º JSON æ–‡ä»¶
        è¿™äº›çŠ¶æ€å±æ€§æœ‰å…¶ä»–è°ƒç”¨è¿›è¡Œç»´æŠ¤ä»¥åŠä¿®æ”¹

        å½“å‰ç‰ˆæœ¬å¯¹äºçŠ¶æ€æ–‡ä»¶è¯»å†™ä¼šæ¯”è¾ƒé¢‘ç¹,å¹¶ä¸”éšç€ä¸‹è½½çš„urlæ•°é‡è¶Šæ¥è¶Šå¤š,æ‰§è¡Œçš„é€Ÿåº¦ä¹Ÿä¼šè¶Šæ¥è¶Šæ…¢,å› ä¸ºçŠ¶æ€çš„ä¿¡æ¯é‡ä¼šéšç€ä¸‹è½½æ•°é‡çš„å¢å¤šè€Œå¢å¤š,å†™å…¥é‡ä¹Ÿè¶Šå¤§,å¯ä»¥è€ƒè™‘ç”¨æ™®é€šæ–‡ç¬”æ–‡ä»¶æˆ–è€…csvæ¥ç®€åŒ–æµç¨‹
        """
        state_file = os.path.join(self.log_dir, "download_state.json")
        try:
            # **ç²¾ç®€ä¼˜åŒ–: åªä¿å­˜ URL é”®çš„åˆ—è¡¨ï¼Œæˆ–ä»¥ URL ä¸ºé”®ï¼ŒTrue ä¸ºå€¼çš„å­—å…¸**
            # state_to_save = {
            #     # è®°å½• URL é›†åˆï¼Œå€¼è®¾ä¸º Trueï¼Œä»¥ä¿æŒå­—å…¸ç»“æ„ï¼Œæ–¹ä¾¿ future-proof
            #     "completed": {url: True for url in self.state["completed"].keys()},
            #     "failed": {url: True for url in self.state["failed"].keys()},
            #     "total_count": self.state["total_count"],
            # }
            state_to_save = self.state
            with open(state_file, "w", encoding="utf-8") as f:
                json.dump(state_to_save, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"ä¿å­˜çŠ¶æ€æ–‡ä»¶å¤±è´¥: {str(e)}", extra={"progress": "SAVE"})

    def _sanitize_filename(self, filename):
        """æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        (é¿å…ç‰¹æ®Šå­—ç¬¦å¯¼è‡´æ–‡ä»¶åç›¸å¯¹æ“ä½œç³»ç»Ÿéæ³•å¯¼è‡´ä¿å­˜å¤±è´¥)
        """
        filename = unicodedata.normalize("NFKD", filename)
        # ç§»é™¤éASCIIå­—ç¬¦(æ­¤æ—¶æ‰€æœ‰å­—ç¬¦éƒ½æ»¡è¶³ç³»ç»Ÿæ–‡ä»¶åçš„è¦æ±‚,ä½†è¿˜è¦æ³¨æ„é•¿åº¦)
        filename = filename.encode("ascii", "ignore").decode("ascii")
        filename = re.sub(r"[^\w\-_.]", "_", filename)
        # é™åˆ¶æ–‡ä»¶åé•¿åº¦200å­—ç¬¦è¿”å›
        return filename[:200]

    def get_progress(self, index):
        """è·å–è¿›åº¦ä¿¡æ¯"""
        return f"{index}/{self.state['total_count']}"

    def _get_proxy_for_worker(self, worker_id=0):
        """ä¸ºæ•´ä¸ªworkeråˆ†é…å›ºå®šä»£ç†ï¼ˆç”¨äºå¤ç”¨contextï¼‰ã€‚"""
        if not self.proxy_configs:
            return None
        return self.proxy_configs[worker_id % len(self.proxy_configs)]

    async def download_url(
        self, context, page, url, index, retry_count=0, worker_id=0, proxy_info=None
    ):
        """ä¸‹è½½å•ä¸ªURL
        æ§åˆ¶æ–‡ä»¶è¾“å‡ºè·¯å¾„ğŸˆ
        """

        # display_url = url[:100] + "..." if len(url) > 100 else url
        display_url = url
        start_time = time.time()

        try:
            # æ–‡ä»¶è·¯å¾„ç”Ÿæˆé€»è¾‘ä¸å˜
            parsed = urlparse(url)
            path = parsed.path[1:] if parsed.path.startswith("/") else parsed.path
            domain_dir = self._sanitize_filename(parsed.netloc)
            base_filename = self._sanitize_filename(path) if path else "index"
            url_hash = str(abs(hash(url)))[:8]
            filename = f"{base_filename}-{url_hash}.html"
            output_path = os.path.join(self.output_dir, domain_dir, filename)

            # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½ (æ–­ç‚¹ç»­ä¼ çš„å…³é”®)
            if url in self.state["completed"]:
                self.logger.info(
                    f"è·³è¿‡å·²å®Œæˆ: {display_url}",
                    extra={"progress": self.get_progress(index)},
                )
                return True

            os.makedirs(os.path.dirname(output_path), exist_ok=True)

            if proxy_info is None:
                proxy_info = "ç›´è¿"

            try:
                # è¯·æ±‚å¼€å§‹æ—¥å¿—
                self.logger.info(
                    f"å¼€å§‹è¯·æ±‚: {display_url} [ä»£ç†: {proxy_info}]",
                    extra={"progress": self.get_progress(index)},
                )

                await page.goto(
                    url, timeout=self.timeout * 1000, wait_until="networkidle"
                )

                content = await page.content()
                elapsed_time = time.time() - start_time
                time_info = f"{elapsed_time:.2f}s"

                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(content)

                try:
                    size_bytes = os.path.getsize(output_path)
                    size_kb = size_bytes / 1024.0
                    size_info = f"{size_kb:.2f} KB"
                except Exception:
                    size_info = "æœªçŸ¥å¤§å°"

                # æ›´æ–°çŠ¶æ€ï¼šåªè®°å½• URL
                self.state["completed"][url] = True
                self.state["failed"].pop(url, None)

                self.state["success_count"] += 1
                self._save_state()

                # æˆåŠŸæ—¥å¿—
                self.logger.info(
                    f"æˆåŠŸä¸‹è½½: {display_url} -> {output_path} [å¤§å°: {size_info}] [è€—æ—¶: {time_info}] [ä»£ç†: {proxy_info}] [æºæ–‡ä»¶: {os.path.basename(self.input_file)}]",
                    extra={"progress": self.get_progress(index)},
                )
            except Exception as e:
                self.logger.warning(f"é¡µé¢å‡ºç°é—®é¢˜: {str(e)}ï¼Œé‡è¯•...")
                raise
            finally:
                pass
            return True

        except Exception as e:
            error_msg = str(e)
            if retry_count < self.max_retries:
                # é‡è¯•é€»è¾‘ä¸å˜
                adjust_delay = min(5, self.delay_range[1] * (retry_count + 1))
                self.logger.warning(
                    f"ä¸‹è½½å¤±è´¥({retry_count + 1}/{self.max_retries}), {error_msg}. å°†åœ¨{adjust_delay:.1f}ç§’åé‡è¯•...",
                    extra={"progress": self.get_progress(index)},
                )
                await asyncio.sleep(adjust_delay)
                return await self.download_url(
                    context,
                    page,
                    url,
                    index,
                    retry_count + 1,
                    worker_id,
                    proxy_info=proxy_info,
                )
            else:
                # æœ€ç»ˆå¤±è´¥ï¼šåªè®°å½• URL
                self.state["failed"][url] = True
                self.state["completed"].pop(url, None)

                self.state["fail_count"] += 1
                self._save_state()
                self.logger.error(
                    f"æœ€ç»ˆå¤±è´¥: {display_url} (é”™è¯¯: {error_msg})",
                    extra={"progress": self.get_progress(index)},
                )
                # è‡ªé€‚åº”é™ä½å¹¶å‘æ•°é€»è¾‘ä¸å˜
                if self.current_concurrency > 1:
                    self.current_concurrency = max(
                        1, int(self.current_concurrency * 0.8)
                    )
                    self.logger.warning(
                        f"é™ä½å¹¶å‘æ•°åˆ° {self.current_concurrency}",
                        extra={"progress": "ADAPTIVE"},
                    )
                return False

    async def worker(self, context, page, queue, worker_id, proxy_info="ç›´è¿"):
        """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
        try:
            while True:
                index, url = await queue.get()
                try:
                    await self.download_url(
                        context,
                        page,
                        url,
                        index,
                        worker_id=worker_id,
                        proxy_info=proxy_info,
                    )
                    if self.delay_range[0] > 0 or self.delay_range[1] > 0:
                        delay = random.uniform(*self.delay_range)
                        await asyncio.sleep(delay)
                finally:
                    queue.task_done()
        finally:
            pass

    async def run(self, urls):
        """è¿è¡Œä¸‹è½½ä»»åŠ¡ğŸˆ
        urlä¸‹è½½ä»»åŠ¡çš„åŸºæœ¬æ‰§è¡Œå•ä½
        """
        self.state["total_count"] = len(urls)

        # é‡æ–°æ ¹æ®çŠ¶æ€æ–‡ä»¶è®¡ç®—æˆåŠŸå’Œå¤±è´¥æ•° (ä»¥é˜²ç”¨æˆ·æ‰‹åŠ¨ä¿®æ”¹çŠ¶æ€æ–‡ä»¶)
        # ç»Ÿè®¡completedå­—æ®µä¸­çš„é”®å€¼å¯¹æ•°é‡ä½œä¸ºè¿›åº¦æ•°é‡(è®¡æ•°å™¨)
        self.state["success_count"] = len(self.state["completed"])
        self.state["fail_count"] = len(self.state["failed"])
        self._save_state()

        # è¿‡æ»¤å·²å®Œæˆå’Œå·²å¤±è´¥çš„ URL
        pending_urls = [
            url
            for url in urls
            if url not in self.state["completed"] and url not in self.state["failed"]
        ]
        pending_count = len(pending_urls)

        processed_count = self.state["success_count"] + self.state["fail_count"]

        self.logger.info(
            f"æ€»URLæ•°: {len(urls)}, å¾…ä¸‹è½½: {pending_count}, å·²å¤„ç†: {processed_count} (æˆåŠŸ {self.state['success_count']}, å¤±è´¥ {self.state['fail_count']})",
            extra={"progress": "INIT"},
        )

        if not pending_urls:
            self.logger.info(
                "æ‰€æœ‰URLå·²ä¸‹è½½å®Œæˆæˆ–å·²å°è¯•å¤„ç†ï¼Œæ— éœ€ç»§ç»­", extra={"progress": "DONE"}
            )
            return

        async with async_playwright() as p:
            launch_options = {
                "headless": self.headless,
                "args": [
                    "--disable-blink-features=AutomationControlled",
                    "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                ],
            }

            browser = await p.chromium.launch(**launch_options)

            if self.proxy_configs:
                proxy_info = "ã€".join([p or "ç›´è¿" for p in self.proxy_configs])
                self.logger.info(
                    f"[CONFIG] ä»£ç†é…ç½®: {proxy_info}", extra={"progress": "CONFIG"}
                )

            queue = asyncio.Queue()
            for index, url in enumerate(urls, 1):
                if url in pending_urls:
                    await queue.put((index, url))

            actual_workers = min(self.max_concurrency, pending_count)
            if actual_workers != self.max_concurrency:
                self.logger.info(
                    f"è¯·æ±‚å¹¶å‘æ•° {self.max_concurrency} å¤§äºå¾…ä¸‹è½½é“¾æ¥æ•° {pending_count}ï¼Œå°†å¹¶å‘æ•°è°ƒæ•´ä¸º {actual_workers}",
                    extra={"progress": "CONFIG"},
                )
            self.current_concurrency = actual_workers

            worker_slots = []
            for i in range(self.current_concurrency):
                worker_proxy = self._get_proxy_for_worker(i)
                if worker_proxy:
                    ctx = await browser.new_context(
                        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                        viewport={"width": 1366, "height": 768},
                        proxy={"server": worker_proxy},
                    )
                    proxy_info = worker_proxy
                else:
                    ctx = await browser.new_context(
                        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                        viewport={"width": 1366, "height": 768},
                    )
                    proxy_info = "ç›´è¿"
                pg = await ctx.new_page()
                worker_slots.append((ctx, pg, proxy_info))

            workers = []
            for i, (ctx, pg, proxy_info) in enumerate(worker_slots):
                workers.append(
                    asyncio.create_task(self.worker(ctx, pg, queue, i, proxy_info))
                )

            await queue.join()

            for w in workers:
                w.cancel()
            await asyncio.gather(*workers, return_exceptions=True)

            for ctx, pg, _ in worker_slots:
                try:
                    await pg.close()
                except:
                    pass
                try:
                    await ctx.close()
                except:
                    pass

            self.logger.info(
                f"ä¸‹è½½å®Œæˆ: æˆåŠŸ {self.state['success_count']}/{len(urls)}, å¤±è´¥ {self.state['fail_count']}",
                extra={"progress": "DONE"},
            )


def read_urls_from_file(file_path):
    """ä»æ–‡ä»¶è¯»å–URLåˆ—è¡¨ï¼Œè‡ªåŠ¨æ£€æµ‹ç¼–ç """
    encodings = ["utf-8", "gbk", "gb2312", "latin1"]
    for enc in encodings:
        try:
            with open(file_path, "r", encoding=enc) as f:
                urls = [
                    line.strip()
                    for line in f
                    if line.strip() and not line.startswith("#")
                ]
            return urls
        except UnicodeDecodeError:
            continue
    raise RuntimeError(f"æ— æ³•ç”¨å¸¸è§ç¼–ç è¯»å–æ–‡ä»¶: {file_path}")


def read_proxies_from_file(file_path):
    """ä»æ–‡ä»¶è¯»å–ä»£ç†åˆ—è¡¨ï¼Œè‡ªåŠ¨æ£€æµ‹ç¼–ç """
    if not file_path:
        return []
    encodings = ["utf-8", "gbk", "gb2312", "latin1"]
    for enc in encodings:
        try:
            with open(file_path, "r", encoding=enc) as f:
                proxies = [
                    line.strip()
                    for line in f
                    if line.strip() and not line.startswith("#")
                ]
            return proxies
        except UnicodeDecodeError:
            continue
        except Exception as e:
            print(f"è¯»å–ä»£ç†æ–‡ä»¶å‡ºé”™: {str(e)}")
            return []
    print(f"æ— æ³•ç”¨å¸¸è§ç¼–ç è¯»å–ä»£ç†æ–‡ä»¶: {file_path}")
    return []


def parse_delay_range(delay_str):
    """è§£æå»¶è¿ŸèŒƒå›´å­—ç¬¦ä¸²"""
    try:
        min_delay, max_delay = map(float, delay_str.split("-"))
        return (min_delay, max_delay)
    except:
        raise argparse.ArgumentTypeError("å»¶è¿ŸèŒƒå›´æ ¼å¼åº”ä¸º'min-max'ï¼Œå¦‚'1.0-3.0'")


def parse_args():
    """è®¾ç½®å¹¶è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="æ™ºèƒ½ç½‘é¡µä¸‹è½½å·¥å…·(æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œè‡ªé€‚åº”ç­–ç•¥)"
    )
    parser.add_argument("input_file", help="åŒ…å«URLåˆ—è¡¨çš„æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "-o",
        "--output",
        default="downloads",
        help="è¾“å‡ºæ ¹ç›®å½• (é»˜è®¤: downloads)ã€‚æ¯ä¸ªè¾“å…¥æ–‡ä»¶å°†åœ¨å…¶ä¸‹åˆ›å»ºå­ç›®å½•ã€‚",
    )
    parser.add_argument(
        "-t", "--timeout", type=int, default=30, help="æ¯ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’) (é»˜è®¤: 30)"
    )
    parser.add_argument(
        "-d",
        "--delay",
        type=parse_delay_range,
        default="1.0-3.0",
        help="è¯·æ±‚ä¹‹é—´çš„éšæœºå»¶è¿ŸèŒƒå›´(ç§’) (é»˜è®¤: 1.0-3.0)",
    )
    parser.add_argument(
        "--headless",
        action="store_true",
        dest="headless",
        help="éšè—æµè§ˆå™¨çª—å£,å³æ— å¤´æ¨¡å¼(é»˜è®¤æ˜¾ç¤º)",
    )
    parser.add_argument(
        "-p",
        "--proxy-file",
        help="åŒ…å«ä»£ç†åˆ—è¡¨çš„æ–‡ä»¶è·¯å¾„(æ¯è¡Œä¸€ä¸ªä»£ç†åœ°å€ï¼Œæ ¼å¼: [protocol://]host:port)",
    )
    parser.add_argument(
        "--allow-direct",
        action="store_true",
        help="å…è®¸ç›´æ¥è¿æ¥ï¼ˆä¸ä½¿ç”¨ä»£ç†ï¼‰ä½œä¸ºä»£ç†åˆ—è¡¨çš„ä¸€ä¸ªé€‰é¡¹",
    )
    parser.add_argument(
        "-c", "--concurrency", type=int, default=3, help="æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 3)"
    )
    parser.add_argument("-r", "--retries", type=int, default=2, help="å¤±è´¥é‡è¯•æ¬¡æ•°")

    args = parser.parse_args()
    return args


def main():
    args = parse_args()
    # æ„é€ è¾“å‡ºè·¯å¾„
    ## åŸºäºè¾“å…¥æ–‡ä»¶ï¼ˆåŒ…å«å¾…ä¸‹è½½çš„urlçš„txtæ–‡æœ¬æ–‡ä»¶ï¼‰ç¡®å®šè¾“å‡ºç›®å½•ï¼Œå®ç°æ–­ç‚¹ç»­ä¼ 
    input_basename = os.path.basename(args.input_file)
    ## ä½¿ç”¨æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºå­ç›®å½•å
    input_name_safe = (
        input_basename.split(".")[0] if "." in input_basename else input_basename
    )
    # æœ€ç»ˆçš„è¾“å‡ºç›®å½•æ˜¯ output_root / input_name_safe
    output_dir = os.path.join(args.output, input_name_safe)

    # è¯»å–URL
    urls = read_urls_from_file(args.input_file)
    if not urls:
        print("é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„URL")
        return

    print(f"å¼€å§‹ä¸‹è½½ {len(urls)} ä¸ªURLåˆ°ç›®å½•: {output_dir}")
    print(f"è®¾ç½®: è¶…æ—¶={args.timeout}s, å»¶è¿Ÿ={args.delay[0]}-{args.delay[1]}s")
    print(
        f"å¹¶å‘æ•°={args.concurrency}, é‡è¯•æ¬¡æ•°={args.retries}, æµè§ˆå™¨çª—å£æ¨¡å¼={'æ˜¾ç¤º' if not args.headless else 'éšè—'}"
    )
    if args.proxy_file:
        print(f"ä»£ç†é…ç½®æ–‡ä»¶: {args.proxy_file}")

    # è¯»å–ä»£ç†åˆ—è¡¨
    proxy_list = []
    if args.proxy_file:
        proxy_list = read_proxies_from_file(args.proxy_file)
        if not proxy_list:
            print("è­¦å‘Š: ä»£ç†æ–‡ä»¶ä¸ºç©ºæˆ–è¯»å–å¤±è´¥")
        else:
            print(f"å·²åŠ è½½ {len(proxy_list)} ä¸ªä»£ç†:")
            for proxy in proxy_list:
                print(f"  - {proxy}")

    # åˆ›å»ºä¸‹è½½å™¨å®ä¾‹
    downloader = WebSourceDownloader(
        output_dir=output_dir,
        timeout=args.timeout,
        delay_range=args.delay,
        headless=args.headless,
        proxy_configs=proxy_list,
        max_concurrency=args.concurrency,
        max_retries=args.retries,
        input_file=args.input_file,
        allow_direct=args.allow_direct,
    )

    # è¿è¡Œä¸‹è½½ä»»åŠ¡
    asyncio.run(downloader.run(urls))


if __name__ == "__main__":
    try:
        import re
        from playwright.async_api import async_playwright
    except ImportError:
        print("è¯·å…ˆå®‰è£…Playwright,è¿è¡Œ: pip install playwright && playwright install")
        exit(1)

    main()
