#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å¤šçº¿ç¨‹å›¾ç‰‡ä¸‹è½½å™¨
Todo: å…è®¸å¿½ç•¥è¯ä¹¦è¿‡æœŸçš„ä¸‹è½½

è¿™ä¸ªæ¨¡å—æä¾›äº†ä¸€ä¸ªé«˜æ•ˆçš„å¤šçº¿ç¨‹å›¾ç‰‡ä¸‹è½½å™¨ï¼Œå¯ä»¥åŒæ—¶ä¸‹è½½å¤šå¼ å›¾ç‰‡ï¼Œ
æ”¯æŒå„ç§å›¾ç‰‡é“¾æ¥æ ¼å¼ï¼Œå¹¶æä¾›ä¸‹è½½ç»Ÿè®¡å’Œæ—¥å¿—åŠŸèƒ½ã€‚

ç”¨æ³•ç¤ºä¾‹:
    # ä½œä¸ºæ¨¡å—å¯¼å…¥ä½¿ç”¨
    from img_downloader import ImageDownloader

    downloader = ImageDownloader(max_workers=10)
    urls = ['http://example.com/image1.jpg', 'https://example.com/image2.png']
    downloader.download(urls, output_dir='./images')

    # æŒ‡å®šæ–‡ä»¶åä¸‹è½½
    name_url_pairs = [('custom_name1.jpg', 'http://example.com/image1.jpg'),
                      ('custom_name2.png', 'https://example.com/image2.png')]
    downloader.download_with_names(name_url_pairs, output_dir='./images')

    # å‘½ä»¤è¡Œä½¿ç”¨
    # python img_downloader.py -i urls.txt -o ./images -w 10
"""
import concurrent.futures
import logging
import os
import random
import re

import subprocess
import shutil
import threading
import time

# from logging import exception
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from filenamehandler import FilenameHandler
from imgcompresser import ImageCompressor

TIMEOUT = 120

IMG_DIR = "./images"
RESIZE_THRESHOLD = 1000, 800  # å›¾ç‰‡å°ºå¯¸å°äºè¿™ä¸ªé˜ˆå€¼åˆ™ä¸è°ƒæ•´åˆ†è¾¨ç‡(å®½*é«˜)
# è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼
LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
# ...existing code...
# åˆ›å»ºå½“å‰æ¨¡å—ä¸“å±çš„æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)  # è®¾ç½®é»˜è®¤æ—¥å¿—çº§åˆ«
fnh = FilenameHandler()
info = logger.info
debug = logger.debug
warning = logger.warning
error = logger.error
exception = logger.exception
# é˜²æ­¢é‡å¤æ·»åŠ  handler
if not logger.handlers:
    # æ§åˆ¶å°æ—¥å¿—å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.NOTSET)  # æ”¹ä¸º NOTSETï¼Œè·Ÿéšloggerçº§åˆ«
    console_formatter = logging.Formatter(LOG_FORMAT)
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)

    # æ–‡ä»¶æ—¥å¿—å¤„ç†å™¨
    try:
        file_handler = logging.FileHandler("img_downloader.log", encoding="utf-8")
        file_handler.setLevel(logging.NOTSET)  # æ”¹ä¸º NOTSETï¼Œè·Ÿéšloggerçº§åˆ«
        file_formatter = logging.Formatter(LOG_FORMAT)
        file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler)
    except Exception as e:
        logger.warning("æ— æ³•åˆ›å»ºæ–‡ä»¶æ—¥å¿—å¤„ç†å™¨: %s", e)
# ...existing code...
# æ–‡ä»¶åå¤„ç†å™¨
fnh = FilenameHandler()
# é…ç½®ä½¿ç”¨çš„User-Agent(è¿‡é•¿å¯ä»¥ç”¨æ‹¬å·åŒ…è£¹é…åˆ+å·åˆ†éš”å­—ç¬¦ä¸²)

# é¢„è®¾å¤šä¸ªå¸¸è§æµè§ˆå™¨çš„ User-AgentğŸˆ
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    #
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.11 "
    "(KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36",
    #
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36",
    #
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36",
]

# è¦æ³¨æ„å¾ˆå¤šå¸¸ç”¨çš„æ™®é€šåˆ†å‰²ç¬¦å·éƒ½ä¸è¦ç”¨äºåˆ†å‰²url,ä¾‹å¦‚åˆ†å·,é€—å·,ç”šè‡³ç©ºæ ¼;
# æ¯”å¦‚urlçš„åè®®åæ¯”å¦‚https://å°±å¸¦æœ‰å†’å·,è€Œé€—å·ä¹Ÿæœ‰é£é™©,éƒ¨åˆ†é“¾æ¥åŒ…å«é€—å·!
# å¦‚æœéœ€è¦,å¯ä»¥è€ƒè™‘åœ¨è°ƒç”¨æ­¤ä¸‹è½½ä¹‹å‰å°†separatorè¿›è¡Œé…ç½®
# æˆ–è€…,å¦‚æœä½ å·²ç»ä½¿ç”¨é€—å·åšurlåˆ†å‰²(è™½ç„¶è¿™ä¸æ˜¯æ¨èæ–¹å¼,è€Œä¸”æœ‰äº›urlæœ¬èº«åŒ…å«é€—å·å°±å®¹æ˜“é€ æˆæ··æ·†),å¯ä»¥äº‹å…ˆå°†`,`æ›¿æ¢ä¸º`>`
URL_SEPARATORS = [
    ">",
    # r"\s+",
    # ";",
    # ",",
]
COMMON_SEPARATORS = [",", ";", r"\s+"]
URL_SEP_PATTERN = "|".join(URL_SEPARATORS)
COMMON_SEP_PATTERN = "|".join(COMMON_SEPARATORS)

URL_SEP_REGEXP = re.compile(URL_SEP_PATTERN)
COMMON_SEP_REGEXP = re.compile(COMMON_SEP_PATTERN)
logger.info("SEP_PATTERN: %s", URL_SEP_PATTERN)
# æœ‰äº›ç½‘ç«™éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®èµ„æºã€‚ä½ å¯ä»¥æ‰‹åŠ¨è·å–ç™»å½•åçš„ Cookieï¼Œå¹¶åœ¨æ¯æ¬¡è¯·æ±‚ä¸­æºå¸¦ã€‚
COOKIES = {"sessionid": "abc123xyz", "csrftoken": "csrf_token_here"}


def download_by_iwr(
    url, output_path, user_agent=None, timeout=TIMEOUT, verify_ssl=True
):
    """
    ä½¿ç”¨ PowerShell çš„ Invoke-WebRequest ä¸‹è½½æŒ‡å®š URL åˆ°æœ¬åœ°æ–‡ä»¶ã€‚

    :param url: ä¸‹è½½é“¾æ¥
    :param output_path: ä¿å­˜åˆ°çš„æœ¬åœ°æ–‡ä»¶è·¯å¾„
    :param user_agent: å¯é€‰ï¼Œè‡ªå®šä¹‰ User-Agent
    :param timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    :param verify_ssl: æ˜¯å¦æ ¡éªŒè¯ä¹¦
    :return: True/False

    """
    # æ„é€  PowerShell å‘½ä»¤
    cmd = [
        # "pwsh",
        "powershell.exe",
        "-NoProfile",
        "-Command",
        '"',
        "Invoke-WebRequest",
        f"-Uri '{url}'",
        f"-OutFile '{output_path}'",
        f"-TimeoutSec {timeout}",
        '"',
    ]
    if user_agent:
        cmd.append(f"-Headers @{{'User-Agent'='{user_agent}'}}")
    if not verify_ssl:
        cmd.append("-SkipCertificateCheck")
    # åˆå¹¶ä¸ºå•è¡Œå­—ç¬¦ä¸²
    ps_command = " ".join(cmd)
    logger.debug("PowerShell å‘½ä»¤: %s", ps_command)
    msg = f"ğŸˆPowerShell å‘½ä»¤:  {ps_command}"
    print(msg)
    try:
        result = subprocess.run(
            ps_command, shell=True, capture_output=True, text=True, check=False
        )
        if result.returncode == 0:
            return True
        else:
            error("Invoke-WebRequest ä¸‹è½½å¤±è´¥: %s", result.stderr.strip())
            return False
    except Exception as e:
        error("è°ƒç”¨ Invoke-WebRequest å¤±è´¥: %s", e)
        return False


def download_by_curl(
    url: str,
    output_path="",
    output_dir="./",
    use_remote_name: bool = False,  # æ–°å¢å‚æ•°ï¼šæ˜¯å¦ä½¿ç”¨è¿œç¨‹æ–‡ä»¶å
    user_agent: str = USER_AGENTS[0],
    timeout: int = TIMEOUT,
    silent: bool = False,
    extra_args: Optional[list] = None,
    reset_cwd=False,  # å‘ç”Ÿå·¥ä½œç›®å½•è½¬æ¢ä¸‹è½½å,æ˜¯å¦å›åˆ°åŸç›®å½•
) -> bool:
    """
        ä½¿ç”¨ç³»ç»Ÿ curl å‘½ä»¤ä¸‹è½½å›¾ç‰‡ï¼ˆæˆ–å…¶ä»–æ–‡ä»¶ï¼‰ã€‚
        (ä½¿ç”¨-kæ¥å¼ºåˆ¶å¿½ç•¥è¯ä¹¦éªŒè¯httpsè¯ä¹¦è¿‡æœŸ)

        Args:
            url (str): è¦ä¸‹è½½çš„æ–‡ä»¶ URLã€‚
            output_path (str): æœ¬åœ°ä¿å­˜è·¯å¾„ã€‚å¦‚æœ use_remote_name ä¸º Trueï¼Œåˆ™åº”ä¸ºä¿å­˜ç›®å½•ã€‚
            output_dir (str): æœ¬åœ°ä¿å­˜ç›®å½•ã€‚å¦‚æœ use_remote_name ä¸º Trueæ—¶æœ‰ç”¨
            user_agent (str): è¯·æ±‚å¤´ä¸­çš„ User-Agent å­—ç¬¦ä¸²ã€‚
            timeout (int): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ã€‚
            silent (bool): æ˜¯å¦é™é»˜æ‰§è¡Œï¼ˆä¸è¾“å‡ºè¿›åº¦ä¿¡æ¯ï¼‰ã€‚
            use_remote_name (bool): æ˜¯å¦ä½¿ç”¨è¿œç¨‹æ–‡ä»¶åä¿å­˜ï¼ˆå³æ·»åŠ  -O å‚æ•°ï¼‰ã€‚
            extra_args (Optional[list]): å…¶ä»–è¦ä¼ ç»™ curl çš„é¢å¤–å‚æ•°åˆ—è¡¨ã€‚


        Returns:
            bool: ä¸‹è½½æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› Falseã€‚

        Raises:
            FileNotFoundError: å¦‚æœ curl ä¸åœ¨ç³»ç»Ÿ PATH ä¸­ã€‚
            PermissionError: å¦‚æœæ²¡æœ‰å†™å…¥ç›®æ ‡è·¯å¾„çš„æƒé™ã€‚
        Examples:
        # ä½¿ç”¨æœåŠ¡å™¨è¿”å›çš„æ–‡ä»¶åå­—
        download_with_curl(
            url=r"https://brigade-hocare.com/5944-large_default/lot-de-2-glissieres-inox-cambro-pour-dw585.jpg",
            use_remote_name=True,
            )
        # æŒ‡å®šä¿å­˜è·¯å¾„
        download_with_curl(
        url=r"https://brigade-hocare.com/5944-large_default/lot-de-2-glissieres-inox-cambro-pour-dw585.jpg",
        output_file=r"C:/Users/Administrator/Pictures/xyz123.jpg"
    )
    """

    cwd = os.getcwd()  # è®°å½•å½“å‰å·¥ä½œç›®å½•
    print(f"å½“å‰å·¥ä½œç›®å½•(curl): {cwd}")
    # æ£€æŸ¥ curl æ˜¯å¦å¯ç”¨
    if not shutil.which("curl"):
        raise FileNotFoundError("curl å‘½ä»¤æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿å·²å®‰è£…å¹¶æ·»åŠ åˆ°ç³»ç»Ÿ PATH")

    # å¦‚æœä¸ä½¿ç”¨è¿œç¨‹æ–‡ä»¶åï¼Œåˆ™ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ï¼Œå¹¶æ‹¼æ¥æ–‡ä»¶å
    if not use_remote_name:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_path)
        os.makedirs(output_dir, exist_ok=True)
    else:
        # ä½¿ç”¨è¿œç¨‹æ–‡ä»¶å,ç¡®è®¤è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        # å°†å·¥ä½œç›®å½•è½¬æ¢åˆ°è¾“å‡ºç›®å½•
        os.chdir(output_dir)
        parsed_url = urlparse(url)
        # è¿™é‡Œè®¡ç®—çš„æ–‡ä»¶åä»…ä¾›å‚è€ƒ
        output_path = os.path.basename(parsed_url.path)
        output_path = os.path.abspath(os.path.join(output_dir, output_path))
        print(f"ä½¿ç”¨è¿œç¨‹æ–‡ä»¶å, è®¡ç®—çš„æ–‡ä»¶å(basenameä¾›å‚è€ƒ): {output_path}")
        # output_file = os.path.basename(url)

    # æ„å»º curl å‘½ä»¤å‚æ•°(åŸºç¡€å‚æ•°,å»ºè®®ç§»åŠ¨åˆ°å‡½æ•°é»˜è®¤å‚æ•°ä¸­)

    cmd = [
        "curl",
        "-f",
        #    , "--retry", "1", "--retry-delay", "2"
    ]
    # å¿½ç•¥è¯ä¹¦å®‰å…¨æ£€æŸ¥
    cmd += ["-k", "--ssl-no-revoke"]

    # æ·»åŠ  User-Agent
    cmd += ["-A", user_agent]

    # æ·»åŠ è¶…æ—¶
    cmd += ["--max-time", str(timeout)]

    # é™é»˜æ¨¡å¼
    if silent:
        cmd += ["--silent"]
    else:
        cmd += ["--progress-bar"]

    # æ·»åŠ  -O å‚æ•°ï¼ˆä½¿ç”¨è¿œç¨‹æ–‡ä»¶åï¼‰
    if use_remote_name:
        cmd += ["-O"]
    else:
        if not output_path:
            raise ValueError("output_path ä¸èƒ½ä¸ºç©º")

        cmd += ["-o", output_path]

    # æ·»åŠ é¢å¤–å‚æ•°
    if extra_args:
        cmd += extra_args

    # æ·»åŠ  URL æœ€å
    cmd += [url]

    try:
        debug(f"æ­£åœ¨ä¸‹è½½: {url}")
        subprocess.run(cmd, check=True)
        if use_remote_name:
            info(f"æ–‡ä»¶å·²ä¿å­˜è‡³(ä»…ä¾›å‚è€ƒ): {output_path}")
        else:
            info(f"æ–‡ä»¶å·²ä¿å­˜è‡³: {output_path}")
        return True
    except subprocess.CalledProcessError as e:
        error(f"curl æ‰§è¡Œå¤±è´¥ï¼Œé”™è¯¯ç : {e.returncode}")
        return False
    except PermissionError as pe:
        raise PermissionError(f"æ— æƒå†™å…¥è·¯å¾„: {output_path}") from pe
    finally:
        # ä¸‹è½½å®Œæˆå,æ˜¯å¦å›åˆ°åŸç›®å½•
        if reset_cwd:
            os.chdir(cwd)  # å›åˆ°åŸç›®å½•
            print(f"å·²å›åˆ°åŸç›®å½•: {cwd}")


class DownloadStatistics:
    """ä¸‹è½½ç»Ÿè®¡ç±»ï¼Œç”¨äºè®°å½•å’Œå±•ç¤ºä¸‹è½½ç»Ÿè®¡ä¿¡æ¯"""

    def __init__(self):
        self.total = 0
        self.success = 0

        # self.progress = 0
        self.task_index = 0
        self.index_lock = threading.Lock()

        self.failed = 0
        self.skipped = 0
        self.start_time = time.time()
        self.end_time = None
        self.failed_urls = []

    def add_success(self):
        """è®°å½•ä¸€æ¬¡æˆåŠŸä¸‹è½½"""
        self.success += 1

    def add_failed(self, url, name=""):
        """è®°å½•ä¸€æ¬¡ä¸‹è½½å¤±è´¥

        è¿™é‡Œå¦‚æœç”¨æˆ·çš„ä¸‹è½½é“¾æ¥åŒæ—¶æŒ‡å®šäº†ä¿å­˜åå­—,åˆ™è¿åŒä¿å­˜çš„åå­—ä¸€èµ·è®°å½•åˆ°å¤±è´¥åˆ—è¡¨ä¸­
        ä¸€èˆ¬æ¥è¯´,å›¾ç‰‡é“¾æ¥ä¸­æå–å‡ºæ¥çš„æ–‡ä»¶åä¸ä¼šå¸¦æœ‰ç©ºæ ¼
        """
        self.failed += 1
        line = f"{name} {url}".strip()
        self.failed_urls.append(line)

    def add_skipped(self):
        """è®°å½•ä¸€æ¬¡è·³è¿‡ä¸‹è½½"""
        self.skipped += 1

    def set_total(self, total: int):
        """è®¾ç½®æ€»ä¸‹è½½æ•°é‡"""
        self.total = total

    def finish(self):
        """å®Œæˆä¸‹è½½ï¼Œè®°å½•ç»“æŸæ—¶é—´"""
        self.end_time = time.time()
        self.save_failed_urls()

    def save_failed_urls(self, file_path="failed_urls.txt"):
        """ä¿å­˜å¤±è´¥çš„URLåˆ°æ–‡ä»¶,ä¾›åç»­æ­¤é‡è¯•"""
        logger.info("Saving failed URLs to %s", file_path)
        logger.info("Failed URLs: [%s]", self.failed_urls)
        with open(file=file_path, mode="w", encoding="utf-8") as f:
            for url in self.failed_urls:
                f.write(url + "\n")

    def get_elapsed_time(self) -> float:
        """è·å–ä¸‹è½½è€—æ—¶ï¼ˆç§’ï¼‰"""
        if self.end_time:
            return self.end_time - self.start_time
        return time.time() - self.start_time

    def get_summary(self) -> Dict[str, Any]:
        """è·å–ä¸‹è½½ç»Ÿè®¡æ‘˜è¦"""
        return {
            "total": self.total,
            "success": self.success,
            "failed": self.failed,
            "skipped": self.skipped,
            "elapsed_time": self.get_elapsed_time(),
            "failed_urls": self.failed_urls,
        }

    def print_summary(self):
        """æ‰“å°ä¸‹è½½ç»Ÿè®¡æ‘˜è¦"""
        summary = self.get_summary()
        logger.info("=" * 50)
        logger.info("ä¸‹è½½ç»Ÿè®¡æ‘˜è¦:")
        logger.info("æ€»è®¡: %d å¼ å›¾ç‰‡", summary["total"])
        logger.info("æˆåŠŸ: %d å¼ å›¾ç‰‡", summary["success"])
        logger.info("ä¸‹è½½æ—¶è·³è¿‡: %d å¼ å›¾ç‰‡", summary["skipped"])
        logger.info("å¤±è´¥: %d å¼ å›¾ç‰‡", summary["failed"])
        logger.info("è€—æ—¶: %.2f ç§’", summary["elapsed_time"])

        if summary["failed"] > 0:
            logger.info("å¤±è´¥çš„URL:")
            for url in summary["failed_urls"]:
                logger.info("  - %s", url)
        logger.info("=" * 50)


class ImageDownloader:
    """å¤šçº¿ç¨‹å›¾ç‰‡ä¸‹è½½å™¨"""

    def __init__(
        self,
        max_workers: int = 10,
        timeout: int = TIMEOUT,
        retry_times: int = 1,
        user_agent: Optional[str] = None,
        cookies: Optional[Dict[str, str]] = None,
        verify_ssl: bool = True,
        proxies=None,
        proxy_strategy="round_robin",
        override=False,
        compress_quality=20,
        quality_rule="",
        output_format="webp",
        remove_original=False,
        use_shutil=False,
        resize_threshold=RESIZE_THRESHOLD,
    ):
        """
        åˆå§‹åŒ–å›¾ç‰‡ä¸‹è½½å™¨

        Args:
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            timeout: ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            retry_times: ä¸‹è½½å¤±è´¥é‡è¯•æ¬¡æ•°
            user_agent: è‡ªå®šä¹‰User-Agent
            cookies: è‡ªå®šä¹‰Cookie
            verify_ssl: æ˜¯å¦éªŒè¯SSLè¯ä¹¦(å¯ç”¨ä¼šæé«˜å®‰å…¨æ€§ï¼Œä½†ä¼šé™ä½ä¸‹è½½é€Ÿåº¦)
            proxies: ä»£ç†è®¾ç½®,æ ¼å¼ä¸º{'http': 'http://proxy.example.com:8080',
                'https': 'https://proxy.example.com:8080'}
            proxy_strategy: ä»£ç†é€‰æ‹©ç­–ç•¥,å¯é€‰å€¼ä¸º'round_robin'ï¼ˆè½®è¯¢ï¼‰å’Œ'random'ï¼ˆéšæœºï¼‰
            compress_quality: å‹ç¼©å›¾ç‰‡è´¨é‡(1-100),å–0è¡¨ç¤ºä¸å‹ç¼©
            quality_rule: å‹ç¼©å›¾ç‰‡è§„åˆ™(é»˜è®¤ä½¿ç”¨ic.compress_imageçš„é»˜è®¤è§„åˆ™)
            output_format: å‹ç¼©å›¾ç‰‡æ ¼å¼(é»˜è®¤ä½¿ç”¨webp)
        """
        self.max_workers = max_workers
        self.timeout = timeout
        self.retry_times = retry_times
        self.verify_ssl = verify_ssl
        self.cookies = cookies
        self.use_shutil = use_shutil
        self.stats = DownloadStatistics()
        self.compress_quality = compress_quality
        self.quality_rule = quality_rule
        self.output_format = output_format
        self.remove_original = remove_original
        self.override = override

        self.ic = ImageCompressor(
            quality_rule=quality_rule,
            remove_original=remove_original,
            resize_threshold=resize_threshold,
        )

        # if retry_times < 1:
        #     warning("retry_times smaller than 1, no retry will be performed.")
        # åˆå§‹åŒ–ä¼šè¯
        self.session = requests.Session()
        # åˆ›å»ºå…·æœ‰é‡è¯•ç­–ç•¥çš„é€‚é…å™¨
        adapter = HTTPAdapter(max_retries=Retry(total=3, backoff_factor=1))
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        if cookies:
            self.session.cookies.update(cookies)
        # è®¾ç½®User-Agent
        self.headers = {
            "User-Agent": user_agent or random.choice(USER_AGENTS),
            "Referer": f"https://{random.choice(['google.com', 'bing.com'])}/",
            "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Accept": "text/html,application/xhtml+xml,application/xml;\
                q=0.9,image/avif,image/webp,*/*;q=0.8",
        }
        self.proxies = proxies or []
        self.proxy_strategy = proxy_strategy
        self.proxy_index = 0

    def get_proxy(self):
        """
        è·å–ä»£ç†é…ç½®ç­–ç•¥
        """
        if not self.proxies:
            return None
        if self.proxy_strategy == "round_robin":
            # è½®è¯¢ä»£ç†(æ›´æ–°ç´¢å¼•)
            proxy = self.proxies[self.proxy_index % len(self.proxies)]
            self.proxy_index += 1
            return proxy
        elif self.proxy_strategy == "random":
            # ç®€å•çš„éšæœºä»£ç†
            return random.choice(self.proxies)
        return None

    def _download_single_image(
        self,
        url: str,
        output_dir: str,
        filename="",
        try_get_ext=True,
        default_ext="",
        retry_gap=1,
        # override=False,
        # compress_quality=20,
        # use_shutil=False,
    ):
        """
        ä¸‹è½½å•å¼ å›¾ç‰‡

        for chunk in response.iter_content(chunk_size=8*2**10):
        ä» HTTP å“åº”ä¸­æŒ‰å—è¯»å–å†…å®¹ï¼Œæ¯å—å¤§å°ä¸º 8192 å­—èŠ‚ï¼ˆ8KBï¼‰ã€‚
        è¿™æ ·åšå¯ä»¥é˜²æ­¢ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªæ–‡ä»¶åˆ°å†…å­˜ï¼Œé€‚åˆå¤§æ–‡ä»¶ä¸‹è½½ã€‚

        Args:
            url: å›¾ç‰‡URL
            output_dir: è¾“å‡ºç›®å½•
            filename: è‡ªå®šä¹‰æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            try_get_ext: å¦‚æœfilenameç¼ºå°‘æ‰©å±•å,æ˜¯å¦å°è·å–æ–‡ä»¶æ‰©å±•å(ä¸ä¿è¯ä¸€å®šè¿”å›æ‰©å±•å)
            override: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨æ–‡ä»¶(å¦‚æœä¸è¦†ç›–åˆ™è·³è¿‡)
            retry_gap: å¤±è´¥é‡è¯•é—´éš”(ç§’)
            compress_quality: å‹ç¼©å›¾ç‰‡è´¨é‡(1-100)

        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        # æ›´æ–°/ä¿®æ”¹å½“å‰ä¸‹è½½çš„è¿›åº¦
        with self.stats.index_lock:
            self.stats.task_index += 1
            # åœ¨é‡Šæ”¾é”ä¹‹å‰,è·å–å½“å‰ä¸‹è½½çš„è¿›åº¦(é€€å‡ºåç”¨self.stats.task_indexè·å–çš„è¿›åº¦å¾€å¾€æ˜¯ä¸æ­£ç¡®çš„)
            current_index = self.stats.task_index
        logger.info(
            "â› downloading(%d/%d): [%s] -> (%s) ",
            current_index,
            self.stats.total,
            url,
            filename,
        )
        # å¦‚æœä¼ å…¥çš„æ–‡ä»¶åæ²¡æœ‰æ‰©å±•å,ä¸”åœ¨try_get_extä¸ºTrueæ—¶,åˆ™[å°è¯•]è¡¥å…¨æ‰©å±•å
        filename = filename.rstrip(".")
        filename = self.prepare_filename(url, filename, try_get_ext, default_ext)
        # debug("filename: [%s]", filename)
        override = self.override
        # é…ç½®ä¸‹è½½ä¸­å¦‚æœå‡ºç°å¤±è´¥çš„é‡è¯•å¾ªç¯(æ¬¡æ•°ç”±retry_timesæŒ‡å®š)
        for attempt in range(self.retry_times + 1):
            # å¦‚æœæŸæ¬¡å°è¯•ä¸‹è½½æˆåŠŸ,åˆ™ç›´æ¥è¿”å›True(ç»“æŸæ­¤ä¸‹è½½ä»»åŠ¡)
            try:
                # # æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºï¼šæ¯æ¬¡è¯·æ±‚å‰æ·»åŠ éšæœºç­‰å¾…,åœ¨å¤±è´¥åæŒ‡æ•°é€€é¿
                # time.sleep(random.uniform(0.5, 2))

                # æ£€æŸ¥responseæ˜¯å¦æ˜¯å›¾ç‰‡ç±»å‹
                # if not self._is_image_response(response=response):
                #     return False

                debug("ä¸‹è½½å›¾ç‰‡: %s (å°è¯• %d/%d)", url, attempt + 1, self.retry_times)
                # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ–‡ä»¶å,åˆ™æŒ‰ç…§é»˜è®¤ç­–ç•¥ç”Ÿæˆæ–‡ä»¶å
                if not filename:
                    filename = fnh.get_filename_from_url(
                        url=url,
                        # response=response,
                        default_ext=default_ext,
                    )
                debug("è·å¾—æ–‡ä»¶åğŸˆ: [%s]", filename)

                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨(å¦‚æœè·¯å¾„å°šä¸å­˜åœ¨åˆ™é€çº§åˆ›å»º,å¦åˆ™ç•¥è¿‡,ä¹Ÿä¸æŠ¥é”™)
                os.makedirs(output_dir, exist_ok=True)

                # ä¿å­˜å›¾ç‰‡(å†™å…¥äºŒè¿›åˆ¶æ–‡ä»¶)ğŸˆ

                file_path = os.path.join(output_dir, filename)
                if os.path.exists(file_path) and not override:
                    logger.info("æ–‡ä»¶å·²å­˜åœ¨,è·³è¿‡: %s", file_path)
                    self.stats.add_skipped()
                    return True
                elif self.use_shutil:
                    res = False
                    if self.use_shutil == "curl":
                        # print("ä½¿ç”¨shutil(curl)ä¸‹è½½å›¾ç‰‡")
                        # ç›®å‰ä½¿ç”¨curlä¸‹è½½å›¾ç‰‡(å°†æ¥å¯èƒ½æ‰©å±•)
                        res = download_by_curl(
                            url=url,
                            output_path=file_path,
                            output_dir=output_dir,
                            timeout=self.timeout,
                            user_agent=self.headers["User-Agent"],
                        )
                    elif self.use_shutil == "iwr":
                        # print("ä½¿ç”¨shutil(iwr)ä¸‹è½½å›¾ç‰‡")
                        res = download_by_iwr(
                            url=url,
                            output_path=file_path,
                            timeout=self.timeout,
                        )
                    if res:
                        self.stats.add_success()
                else:
                    # é€šè¿‡pythonå‘é€getè¯·æ±‚è·å–åŒ…å«æ–‡ä»¶(å›¾ç‰‡)çš„å“åº”
                    # (é…Œæƒ…å¯ç”¨streamå‚æ•°å¯ä»¥å®ç°æµå¼ä¸‹è½½,å‡å°‘å†…å­˜å ç”¨,é…åˆåé¢çš„iter_contentæ–¹æ³•ä½¿ç”¨)
                    response = self.session.get(
                        url=url,
                        timeout=self.timeout,
                        verify=self.verify_ssl,
                        stream=True,
                        # proxies={"https": self.get_proxy()},  # ä½¿ç”¨ä»£ç†
                    )
                    response.raise_for_status()

                    self.download_by_py(url, response=response, file_path=file_path)
                    self.stats.add_success()
                # æ‰§è¡Œå‹ç¼©ä»»åŠ¡
                quality = self.compress_quality
                if quality or self.quality_rule:
                    # åˆ¤æ–­ä¸‹è½½çš„å›¾ç‰‡å¤§å°æ˜¯å¦é«˜äº100KB,å¦‚æœæ˜¯åˆ™å‹ç¼©å›¾ç‰‡
                    # if os.path.getsize(file_path) < 100 * 2**10:
                    #     debug("å›¾ç‰‡å°äº100KB,ä½¿ç”¨é«˜qualityå‹ç¼©")
                    #     quality = 70

                    # else:
                    print("å°è¯•å‹ç¼©å›¾ç‰‡")
                    self.ic.compress_image(
                        input_path=file_path,
                        # output_path=file_path,
                        output_format=self.output_format,
                        quality=quality,
                        overwrite=True,
                    )
                # return True
                return file_path

            except requests.exceptions.RequestException as e:
                # å¦‚æœæ˜¯åº”ä¸ºè¯·æ±‚å¼‚å¸¸å¯¼è‡´çš„ä¸‹è½½å¤±è´¥,è¿™åœ¨è¿™é‡Œæ•è·;
                warning(
                    "ä¸‹è½½å¤±è´¥ (å°è¯• %d/%d): %s, é”™è¯¯: %s",
                    attempt + 1,
                    self.retry_times,
                    url,
                    str(e),
                )
                # å¦‚æœè¿˜æœ‰é‡è¯•çš„æœºä¼š,åˆ™ç­‰å¾…ä¸€æ®µæ—¶é—´åå›åˆ°å¾ªç¯å†é‡è¯•
                if attempt < self.retry_times - 1:
                    wait_time = retry_gap * (2**attempt) + random.uniform(0, 1)
                    time.sleep(wait_time)
                else:
                    # å°è¯•æ¬¡æœºä¼šç”¨å®Œ,ç›´æ¥æŠ¥é”™å¹¶è¿”å›False
                    wait_time = retry_gap * (2**attempt) + random.uniform(0, 1)
                    time.sleep(wait_time)
                    error("ä¸‹è½½å¤±è´¥: %s, é”™è¯¯: %s", url, str(e))
                    self.stats.add_failed(url, name=filename or "")
                    return False

        return False

    def prepare_filename(self, url, filename, try_get_ext, default_ext):
        """å‡†å¤‡æ–‡ä»¶å,ç”¨äºæŒ‡å®šä¸‹è½½ä¿å­˜æ–‡ä»¶"""
        if filename:
            _, ext = os.path.splitext(filename)
            # ext = ext.strip(".")
            if not ext:
                debug("æŒ‡å®šæ–‡ä»¶åç¼ºå°‘æ‰©å±•å")
                if try_get_ext:
                    raw_name = filename
                    filename = self.complete_extension(
                        filename=filename, url=url, default_ext=default_ext
                    )
                    debug(
                        "å·²å°è¯•è¡¥å…¨æ–‡ä»¶æ‰©å±•å: %s -> %s",
                        raw_name,
                        filename,
                    )
            else:
                debug("æ–‡ä»¶ååŒ…å«æ‰©å±•å:%s", ext)
        else:
            debug("æœªæŒ‡å®šæ–‡ä»¶å,å°è¯•ä»URLä¸­è·å–")
        return filename

    def download_by_py(self, url, response, file_path):
        """ä½¿ç”¨pythonçš„åº“ä¸‹è½½å›¾ç‰‡(æ“ä½œæ¯”è¾ƒåŸå§‹å’Œåº•å±‚)"""
        with open(file_path, "wb") as f:
            # åˆ†å—å†™å…¥å“åº”å†…å®¹
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        file_size = os.path.getsize(file_path)
        logger.info("æˆåŠŸä¸‹è½½: %s -> %s (%d å­—èŠ‚)", url, file_path, file_size)

    def _is_image_response(self, response):
        """æ£€æŸ¥responseæ˜¯å¦æ˜¯å›¾ç‰‡ç±»å‹"""
        content_type = response.headers.get("Content-Type", "")
        if not content_type.startswith("image/"):
            # error("å“åº”ä¸æ˜¯å›¾ç‰‡ç±»å‹: %s -> Content-Type=%s", url, content_type)
            # self.stats.add_failed(url=url, name=filename or "")
            return False

    def complete_extension(self, filename, url, default_ext=""):
        """è¡¥å…¨æ–‡ä»¶æ‰©å±•å

        Args:
            url:æ–‡ä»¶èµ„æºçš„url(å½“filenameç¼ºå°‘æ‰©å±•åæ—¶,å°è¯•ä»å¯¹åº”æ–‡ä»¶çš„urlä¸­è·å–)

        å¦‚æœè¾“å…¥çš„filenameæ²¡æœ‰æ‰©å±•å,åˆ™å°è¯•è¡¥å…¨æ‰©å±•å
        å¦‚æœå·²ç»æœ‰æ‰©å±•å,åˆ™è¿”å›åŸæœ¬åœ°filename
        å¦‚æœæ–‡ä»¶åä¸ºç©º,åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²

        """
        _, ext = os.path.splitext(p=filename or "")
        debug("filename: [%s], ext:[ %s]", filename, ext)
        if not ext:
            # æ–‡ä»¶åéç©ºä½†æ˜¯æ‰©å±•åä¸ºç©ºæ—¶å°è¯•è·å–æ‰©å±•å
            if filename:
                ext = fnh.get_file_extension(url=url, default_ext=default_ext)
                filename = f"{filename}{ext}"
            else:
                debug("ç¼ºå°‘æ–‡ä»¶åå’Œæ‰©å±•å")
        return filename

    def download_only_url(
        self, urls: List[Any], output_dir: str = IMG_DIR, default_ext=""
    ) -> Dict[str, Any]:
        """
        ä»…æ ¹æ®ç»™å®šçš„urlä¸‹è½½å›¾ç‰‡

        å¦ä¸€ä¸ªå‡½æ•°download_with_names()å¯ä»¥æŒ‡å®šè‡ªå®šä¹‰æ–‡ä»¶åä¸‹è½½å›¾ç‰‡

        Args:
            urls: å›¾ç‰‡URLåˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            Dict: ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("å¼€å§‹ä¸‹è½½ %d å¼ å›¾ç‰‡åˆ° %s", len(urls), output_dir)

        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.stats = DownloadStatistics()
        self.stats.set_total(len(urls))

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # ä½¿ç”¨çº¿ç¨‹æ± å¤šçº¿ç¨‹ä¸‹è½½å›¾ç‰‡
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=self.max_workers
        ) as executor:
            future_to_url = {
                executor.submit(
                    self._download_single_image,
                    url,
                    output_dir,
                    default_ext=default_ext,
                ): url
                for url in urls
            }

            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    future.result()

                except Exception as e:
                    exception("å¤„ç†ä¸‹è½½æ—¶å‘ç”Ÿå¼‚å¸¸: %s, é”™è¯¯: %s", url, str(e))
                    self.stats.add_failed(url)

        # å®Œæˆä¸‹è½½ï¼Œæ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.stats.finish()
        self.stats.print_summary()

        return self.stats.get_summary()

    def download_with_names(
        self,
        name_url_pairs: List[Any],
        output_dir: str = IMG_DIR,
        default_ext="",
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨è‡ªå®šä¹‰æ–‡ä»¶åä¸‹è½½å¤šå¼ å›¾ç‰‡
        å¦ä¸€ä¸ªå‡½æ•°ï¼šdownload_only_url()ä»…æ ¹æ®urlä¸‹è½½å›¾ç‰‡

        Args:
            name_url_pairs: (æ–‡ä»¶å, URL)å…ƒç»„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            Dict: ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("å¼€å§‹ä¸‹è½½ %d å¼ å›¾ç‰‡åˆ° %s", len(name_url_pairs), output_dir)

        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.stats = DownloadStatistics()
        self.stats.set_total(len(name_url_pairs))

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(name=output_dir, exist_ok=True)

        # ä½¿ç”¨çº¿ç¨‹æ± ä¸‹è½½å›¾ç‰‡
        with concurrent.futures.ThreadPoolExecutor(
            max_workers=self.max_workers
        ) as executor:
            # ä½¿ç”¨å­—å…¸è§£æå¼åˆ›å»ºå’Œå­˜å‚¨ä»»åŠ¡{future: (filename, url)}
            future_to_pair = {
                executor.submit(
                    self._download_single_image,
                    url=url,
                    output_dir=output_dir,
                    filename=filename,  # æ­¤å‚æ•°å–å€¼æ¥è‡ªå¯¹å¯è¿­ä»£å¯¹è±¡name_url_pairsè§£æå‡ºæ¥çš„filename,urlå…ƒç»„ä¸­çš„ç¬¬ä¸€ä¸ªåˆ†é‡
                    default_ext=default_ext,
                ): (filename, url)
                for filename, url in name_url_pairs
            }

            for future in concurrent.futures.as_completed(fs=future_to_pair):
                filename, url = future_to_pair[future]
                try:
                    future.result()
                    # success = future.result()
                    # if success:
                    #     self.stats.add_success()
                    # else:
                    #     self.stats.add_failed(url)
                except Exception as e:
                    failed_dict = {filename: url}
                    exception("å¤„ç†%sä¸‹è½½æ—¶å‘ç”Ÿå¼‚å¸¸, é”™è¯¯:%s", failed_dict, str(e))
                    self.stats.add_failed(url=url, name=filename)

        # å®Œæˆä¸‹è½½ï¼Œæ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.stats.finish()
        self.stats.print_summary()

        return self.stats.get_summary()


##
if __name__ == "__main__":
    logger.info("Welcome to use this image downloader module!")
