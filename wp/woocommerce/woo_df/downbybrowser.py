import asyncio
import logging
import os
import random
import re
import shutil
import unicodedata
from typing import Any, Dict, List, Optional, Tuple, Union
from imgcompresser import ImageCompressor  # å‡è®¾ imgcompresser.py åœ¨åŒä¸€ç¯å¢ƒ
from urllib.parse import urlparse
from playwright.async_api import BrowserContext, Page, async_playwright

# é…ç½®ä¸€ä¸ªåŸºæœ¬çš„loggerï¼Œé¿å…åœ¨å¤–éƒ¨è°ƒç”¨æ—¶æ²¡æœ‰handler
logger = logging.getLogger("browser_downloader")
if not logger.handlers:
    logger.setLevel(logging.INFO)
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        "%(asctime)s [%(levelname)s] %(message)s", datefmt="%H:%M:%S"
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)

# --- è¾…åŠ©å‡½æ•° ---


def _sanitize_filename(filename: str) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦ï¼Œä¿è¯è·¨å¹³å°å…¼å®¹æ€§ã€‚"""
    filename = unicodedata.normalize("NFKD", filename)
    filename = filename.encode("ascii", "ignore").decode("ascii")
    # å…è®¸çš„å­—ç¬¦æ˜¯å­—æ¯æ•°å­—, ä¸‹åˆ’çº¿, è¿å­—ç¬¦, ç‚¹å·
    filename = re.sub(r"[^\w\-_.]", "_", filename)
    return filename[:200].strip()


def _guess_filename_from_url(url: str, default_name: str = "index") -> str:
    """ä» URL ä¸­çŒœæµ‹æ–‡ä»¶åã€‚"""
    parsed = urlparse(url)
    path = parsed.path

    # å°è¯•ä»è·¯å¾„ä¸­è·å–æ–‡ä»¶å
    filename = os.path.basename(path)
    if not filename or filename == "/":
        filename = default_name

    # ç¡®ä¿æœ‰æ‰©å±•åï¼Œå¦‚æœ URL ä¸­æ²¡æœ‰ï¼ŒPlaywright ä¸‹è½½çš„é»˜è®¤è¡Œä¸ºæ˜¯æ ¹æ® Content-Type
    # ä½†æ­¤å¤„æˆ‘ä»¬æ— æ³•æå‰çŸ¥é“ Content-Typeï¼Œå› æ­¤å¦‚æœ URL è·¯å¾„ä¸­æ²¡æœ‰ç‚¹å·ï¼Œåˆ™å‡å®šä¸º .html
    if "." not in filename:
        filename = filename + ".html"

    return _sanitize_filename(filename)


class BrowserDownloader:
    """
    åŸºäº Playwright çš„ç½‘ç»œèµ„æºä¸‹è½½å™¨ã€‚
    ä¸“ç”¨äºä¸‹è½½é‚£äº›éœ€è¦å®Œæ•´æµè§ˆå™¨ç¯å¢ƒæ‰èƒ½è·å–çš„èµ„æº (å¦‚åŠ¨æ€åŠ è½½çš„å›¾ç‰‡æˆ–å—ä¿æŠ¤çš„ç½‘é¡µå†…å®¹)ã€‚
    æ”¯æŒæŒ‡å®šè¾“å‡ºè·¯å¾„ã€ä»£ç†é…ç½®å’Œå¹¶å‘æ§åˆ¶ã€‚
    """

    def __init__(
        self,
        headless: bool = True,
        timeout: int = 30,
        delay_range: Tuple[float, float] = (0, 0),
        max_concurrency: int = 3,
        max_retries: int = 1,
        # å›¾ç‰‡å‹ç¼©ç›¸å…³å‚æ•°
        ic: ImageCompressor | None = None,
        compress_quality: int = 0,
        output_format: str = "webp",
    ):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨ã€‚

        Args:
            headless: æ˜¯å¦å¯ç”¨æ— å¤´æ¨¡å¼ (ä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£)ã€‚
            timeout: å•æ¬¡è¯·æ±‚çš„è¶…æ—¶æ—¶é—´ (ç§’)ã€‚
            delay_range: æ¯æ¬¡ä¸‹è½½ä»»åŠ¡ä¹‹é—´éšæœºå»¶è¿Ÿçš„æ—¶é—´èŒƒå›´ (min, max)ã€‚
            max_concurrency: æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ã€‚
            max_retries: å•ä¸ª URL ä¸‹è½½å¤±è´¥åçš„æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚
            ic: å›¾ç‰‡å‹ç¼©å™¨å®ä¾‹ã€‚
            compress_quality: å›¾ç‰‡å‹ç¼©è´¨é‡ (0-100)ã€‚0 è¡¨ç¤ºä¸å‹ç¼© (æˆ–éµå¾ª quality_rule)ã€‚
            quality_rule: åŸºäºæ–‡ä»¶å¤§å°çš„å‹ç¼©è§„åˆ™ (å¦‚ '1M=80, 500K=90')ã€‚
            output_format: å›¾ç‰‡å‹ç¼©åçš„è¾“å‡ºæ ¼å¼ (å¦‚ 'webp', 'jpeg')ã€‚
            remove_original: å›¾ç‰‡å‹ç¼©åæ˜¯å¦ç§»é™¤åŸå§‹æ–‡ä»¶ã€‚
            resize_threshold: ä»…å½“å›¾ç‰‡å®½åº¦æˆ–é«˜åº¦è¶…è¿‡æ­¤é˜ˆå€¼æ—¶æ‰è¿›è¡Œè°ƒæ•´å¤§å° (å®½, é«˜)ã€‚
            fake_format: æ˜¯å¦å…è®¸å‹ç¼©æ—¶ä¼ªé€ æ–‡ä»¶åæ‰©å±•å (å¦‚å°† JPG å‹ç¼©ä¸º WEBP ä½†ä¿ç•™ .jpg æ‰©å±•)ã€‚
        """
        self.headless = headless
        self.timeout = timeout
        self.delay_range = delay_range
        self.max_concurrency = max_concurrency
        self.max_retries = max_retries
        # å›¾ç‰‡å‹ç¼©ç›¸å…³å‚æ•°
        self.ic = ic
        self.compress_quality = compress_quality
        self.output_format = output_format

    @staticmethod
    def _read_proxies(proxy_input: Optional[Union[str, List[str]]]) -> List[str]:
        """
        ä»æ–‡ä»¶è·¯å¾„æˆ–ç›´æ¥çš„ä»£ç†å­—ç¬¦ä¸²ä¸­è¯»å–ä»£ç†åˆ—è¡¨ã€‚
        ä¸€ä¸ª None æˆ–ç©ºåˆ—è¡¨/ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºç›´è¿(å–å†³äºç¯å¢ƒä»£ç†)ã€‚
        """
        if not proxy_input:
            return []

        if isinstance(proxy_input, str):
            if os.path.exists(proxy_input):
                # è®¤ä¸ºæ˜¯æ–‡ä»¶è·¯å¾„
                proxies = []
                try:
                    with open(proxy_input, "r", encoding="utf-8") as f:
                        for line in f:
                            stripped = line.strip()
                            if stripped and not stripped.startswith("#"):
                                proxies.append(stripped)
                    return proxies
                except Exception as e:
                    logger.error(f"è¯»å–ä»£ç†æ–‡ä»¶å¤±è´¥: {proxy_input}, é”™è¯¯: {e}")
                    return []
            else:
                # è®¤ä¸ºæ˜¯å•ä¸ªä»£ç†å­—ç¬¦ä¸²
                return [proxy_input]

        # è®¤ä¸ºå·²ç»æ˜¯ä»£ç†åˆ—è¡¨
        return [p for p in proxy_input if p]

    def _get_proxy_config(
        self, proxy_list: List[str], worker_id: int
    ) -> Optional[Dict[str, str]]:
        """æ ¹æ® worker_id å’Œä»£ç†åˆ—è¡¨è·å– Playwright ä»£ç†é…ç½®ã€‚"""
        if not proxy_list:
            return None

        proxy_url = proxy_list[worker_id % len(proxy_list)]
        return {"server": proxy_url}

    async def _download_single_url(
        self,
        page: Page,
        url: str,
        output_path: str,
        user_agent: Optional[str] = None,
        retry_count: int = 0,
        proxy_info: str = "ç›´è¿",
        progress_info: Optional[str] = None,
    ) -> bool:
        """
        æ ¸å¿ƒä¸‹è½½é€»è¾‘ï¼šä¸‹è½½å•ä¸ª URL å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ (å¤ç”¨ Page)ã€‚
        ä¸‹è½½å®Œæˆåè‡ªåŠ¨è¿›è¡Œå›¾ç‰‡å‹ç¼©å¤„ç†ï¼ˆå¦‚å¯ç”¨ï¼‰ã€‚
        """
        start_time = asyncio.get_event_loop().time()
        display_url = url

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if (
            os.path.exists(output_path)
            and os.path.getsize(output_path) > 0
            and retry_count == 0
        ):
            logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {output_path}", extra={"progress": "SKIP"})
            return True

        try:
            logger.info(
                f"ğŸš€{progress_info if progress_info else ''} å¼€å§‹è¯·æ±‚: {display_url} [ä»£ç†: {proxy_info}] -> {output_path} ".strip()
            )

            # ä½¿ç”¨å¤ç”¨çš„ page å‘èµ·è¯·æ±‚
            # wait_until="networkidle" ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½/åŠ¨æ€èµ„æºåŠ è½½
            response = await page.goto(
                url, timeout=self.timeout * 1000, wait_until="networkidle"
            )

            if not response:
                raise Exception("æœªè·å–åˆ°æœ‰æ•ˆå“åº” (Response is None)")

            if response.status >= 400:
                if response.status == 404:
                    logger.error(f"404 æœªæ‰¾åˆ°: {display_url}ï¼Œç›´æ¥æ”¾å¼ƒä¸é‡è¯•ã€‚", extra={"progress": "FAIL"})
                    return False
                else:
                    raise Exception(f"HTTP çŠ¶æ€ç é”™è¯¯: {response.status}")

            content_type = (
                response.headers.get("content-type", "").split(";")[0].strip().lower()
            )

            # è·å–åŸå§‹æ•°æ®æµ
            data_to_write = await response.body()

            # å†™å…¥æ–‡ä»¶
            with open(output_path, "wb") as f:
                f.write(data_to_write)

            # è®¡ç®—å¤§å°å’Œè€—æ—¶
            elapsed_time = asyncio.get_event_loop().time() - start_time
            size_bytes = os.path.getsize(output_path)
            size_info = (
                f"{size_bytes / 1024.0:.2f} KB"
                if size_bytes >= 1024
                else f"{size_bytes} B"
            )
            time_info = f"{elapsed_time:.2f}s"

            logger.info(
                f"{progress_info if progress_info else ''} æˆåŠŸä¸‹è½½: {display_url} [ç±»å‹: {content_type}] [å¤§å°: {size_info}] [è€—æ—¶: {time_info}] ".strip()
            )

            # --- å›¾ç‰‡å‹ç¼©å¤„ç†ï¼ˆå¦‚å¯ç”¨ï¼‰---
            # ä»…å¯¹å›¾ç‰‡ç±»å‹è¿›è¡Œå‹ç¼©
            if self.ic and content_type.startswith("image/"):
                try:
                    logger.info(f"å°è¯•å‹ç¼©å›¾ç‰‡: {output_path}")
                    self.ic.compress_image(
                        input_path=output_path,
                        output_format=self.output_format,
                        quality=self.compress_quality,
                        overwrite=True,
                    )
                    logger.info(f"å›¾ç‰‡å‹ç¼©å®Œæˆ: {output_path}")
                except Exception as e:
                    logger.warning(f"å›¾ç‰‡å‹ç¼©å¤±è´¥: {output_path}, é”™è¯¯: {e}")

            return True

        except Exception as e:
            error_msg = str(e)
            if retry_count < self.max_retries:
                adjust_delay = self.delay_range[1] * (retry_count + 1)
                logger.warning(
                    f"ä¸‹è½½å¤±è´¥({retry_count + 1}/{self.max_retries}), {error_msg}. å°†åœ¨{adjust_delay:.1f}ç§’åé‡è¯•...",
                    extra={"progress": f"RETRY {retry_count + 1}"},
                )
                await asyncio.sleep(adjust_delay)
                # é‡è¯•æ—¶ç»§ç»­ä½¿ç”¨åŒä¸€ä¸ª page å¯¹è±¡
                return await self._download_single_url(
                    page, url, output_path, user_agent, retry_count + 1, proxy_info
                )
            else:
                logger.error(
                    f"æœ€ç»ˆå¤±è´¥: {display_url} (é”™è¯¯: {error_msg})",
                    extra={"progress": "FAIL"},
                )
                return False

    async def _worker(
        self,
        page: Page,
        queue: asyncio.Queue,
        proxy_info: str,
        completed_count: List[int],
        total_tasks: int,
        lock: asyncio.Lock,
    ) -> None:
        """å·¥ä½œçº¿ç¨‹ï¼Œå¤ç”¨ page å¯¹è±¡ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡å¹¶æ‰§è¡Œä¸‹è½½ã€‚è¿›åº¦ä¿¡æ¯èåˆåˆ°è¯·æ±‚æç¤ºè¯­å¥ã€‚"""
        while True:
            try:
                url, output_path, user_agent = await queue.get()
                progress_info = None
                async with lock:
                    completed_count[0] += 1
                    progress_info = f"[{completed_count[0]}/{total_tasks}]"
                try:
                    await self._download_single_url(
                        page,
                        url,
                        output_path,
                        user_agent,
                        proxy_info=proxy_info,
                        progress_info=progress_info,
                    )
                    if self.delay_range[0] > 0 or self.delay_range[1] > 0:
                        delay = random.uniform(*self.delay_range)
                        await asyncio.sleep(delay)
                finally:
                    queue.task_done()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"å·¥ä½œçº¿ç¨‹å¼‚å¸¸: {e}")

    async def _run_async(
        self,
        tasks: List[Tuple[str, str, Optional[str]]],
        proxy_input: Optional[Union[str, List[str]]] = None,
    ):
        """å¼‚æ­¥è¿è¡Œå¹¶å‘ä¸‹è½½ä»»åŠ¡ï¼Œå®ç° Context å’Œ Page å¤ç”¨ã€‚"""
        proxy_list = self._read_proxies(proxy_input)
        proxy_configs = proxy_list if proxy_list else [""]

        logger.info(
            f"é…ç½®: å¹¶å‘={self.max_concurrency}, ä»£ç†æ± å¤§å°={len(proxy_configs)}"
        )

        async with async_playwright() as p:
            launch_options: Dict[str, Any] = {
                "headless": self.headless,
                "args": [
                    "--disable-blink-features=AutomationControlled",
                ],
            }

            browser = await p.chromium.launch(**launch_options)

            queue: asyncio.Queue[Tuple[str, str, Optional[str]]] = asyncio.Queue()
            for task in tasks:
                await queue.put(task)

            actual_workers = min(self.max_concurrency, len(tasks))
            worker_slots: List[Tuple[BrowserContext, Page, str]] = []

            for i in range(actual_workers):
                worker_proxy_url = proxy_configs[i % len(proxy_configs)]
                default_user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                context_args: Dict[str, Any] = {
                    "user_agent": default_user_agent,
                    "viewport": {"width": 1366, "height": 768},
                }
                if worker_proxy_url:
                    context_args["proxy"] = {"server": worker_proxy_url}
                    p_info = worker_proxy_url
                else:
                    p_info = "ç¯å¢ƒä»£ç†"
                ctx = await browser.new_context(**context_args)
                pg = await ctx.new_page()
                worker_slots.append((ctx, pg, p_info))

            # è¿›åº¦è®¡æ•°å™¨å’Œé”
            completed_count = [0]  # ç”¨åˆ—è¡¨åŒ…è£¹ä»¥ä¾¿å¯å˜
            total_tasks = len(tasks)
            lock = asyncio.Lock()

            workers = []
            for ctx, pg, p_info in worker_slots:
                workers.append(
                    asyncio.create_task(
                        self._worker(
                            pg, queue, p_info, completed_count, total_tasks, lock
                        )
                    )
                )

            await queue.join()

            for w in workers:
                w.cancel()
            await asyncio.gather(*workers, return_exceptions=True)

            for ctx, pg, _ in worker_slots:
                try:
                    await pg.close()
                    await ctx.close()
                except:
                    pass

            logger.info("æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å·²å®Œæˆã€‚")

    def batch_download(
        self,
        tasks: List[Tuple[str, str]],
        proxy_input: Optional[Union[str, List[str]]] = None,
        output_dir: Optional[str] = None,
    ):
        """
        [å…¬å…±æ–¹æ³•] é€šè¿‡ Playwright æµè§ˆå™¨ç¯å¢ƒæ‰¹é‡ä¸‹è½½ç½‘ç»œèµ„æºã€‚

        æ‰€æœ‰ä¸‹è½½å‚æ•° (headless, timeout, concurrency, retries, delay) ç»§æ‰¿è‡ª Downloader å®ä¾‹ã€‚

        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (url, output_path) çš„å…ƒç»„ã€‚
            proxy_input: ä»£ç†è¾“å…¥ï¼Œå¯ä»¥æ˜¯å•ä¸ªä»£ç†å­—ç¬¦ä¸²ã€ä»£ç†åˆ—è¡¨æˆ–ä»£ç†æ–‡ä»¶è·¯å¾„ã€‚
            output_dir: å¯é€‰ï¼Œå¦‚æœæŒ‡å®šï¼Œå°†æ‰€æœ‰ output_path ä»…ä¸ºæ–‡ä»¶åçš„ä»»åŠ¡è¡¥å…¨ä¸º output_dir/filenameã€‚
        Returns:
            True è¡¨ç¤ºä¸‹è½½è¿‡ç¨‹å®Œæˆ (ä¸ä»£è¡¨æ‰€æœ‰ä»»åŠ¡æˆåŠŸ)ã€‚
        """
        if not tasks:
            logger.warning("ä»»åŠ¡åˆ—è¡¨ä¸ºç©ºï¼Œæ— éœ€ä¸‹è½½ã€‚")
            return True

        # å¦‚æœæŒ‡å®šäº† output_dirï¼Œåˆ™å°†æ‰€æœ‰ output_path ä»…ä¸ºæ–‡ä»¶åçš„ä»»åŠ¡è¡¥å…¨ä¸º output_dir/filename
        processed_tasks: List[Tuple[str, str, Optional[str]]] = []
        for url, output_path in tasks:
            # åˆ¤æ–­ output_path æ˜¯å¦ä¸ºç®€å•çš„æ–‡ä»¶åï¼ˆæ²¡æœ‰ç›®å½•åˆ†éš”ç¬¦ä¸”éç»å¯¹è·¯å¾„ï¼‰
            if (
                output_dir
                and (not os.path.isabs(output_path))
                and (os.path.dirname(output_path) == "")
            ):
                # ä»…ä¸ºæ–‡ä»¶åï¼Œè¡¥å…¨ä¸º output_dir/filename
                output_path = os.path.join(output_dir, output_path)
            processed_tasks.append((url, output_path, None))  # None for user_agent

        logger.info(f"--- å¯åŠ¨æ‰¹é‡é“¾æ¥ä¸‹è½½ä»»åŠ¡ (æ€»æ•°: {len(processed_tasks)}) ---")

        try:
            # ä½¿ç”¨ self._run_async å¯åŠ¨å¼‚æ­¥ä¸‹è½½
            asyncio.run(self._run_async(processed_tasks, proxy_input))
        except KeyboardInterrupt:
            logger.warning("ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­ã€‚")
        except Exception as e:
            logger.error(f"ä¸‹è½½ä»»åŠ¡å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
        return True

    def single_download(
        self,
        url: str,
        output_path: str = "",
        use_remote_name: bool = False,
        output_dir_for_remote_name: str = "./",
        user_agent: Optional[str] = None,
        proxy_input: Optional[Union[str, List[str]]] = None,
        # å•ä¸ªä¸‹è½½æ—¶ï¼Œå¯ä»¥ä¸´æ—¶è¦†ç›–é‡è¯•æ¬¡æ•°
        retries: Optional[int] = None,
    ) -> None:
        """
        [å…¬å…±æ–¹æ³•] é€šè¿‡ Playwright æµè§ˆå™¨ç¯å¢ƒä¸‹è½½å•ä¸ªç½‘ç»œèµ„æºã€‚

        æ³¨æ„: æ­¤æ–¹æ³•å°†ä¸´æ—¶è®¾ç½® max_concurrency=1 ä¸” delay_range=(0.0, 0.0) æ‰§è¡Œä¸‹è½½ã€‚

        Args:
            url: å¿…é¡»å¡«å†™çš„è¦è¯·æ±‚çš„ URLã€‚
            output_path: æŒ‡å®šä¿å­˜æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚å¦‚æœæä¾›ï¼Œå®ƒå°†è¦†ç›– use_remote_name çš„é€»è¾‘ã€‚
            use_remote_name: æ˜¯å¦ä½¿ç”¨ URL çŒœæµ‹çš„æ–‡ä»¶åä½œä¸ºä¿å­˜æ–‡ä»¶åã€‚
            output_dir_for_remote_name: å¦‚æœ use_remote_name ä¸º Trueï¼ŒæŒ‡å®šä¿å­˜çš„ç›®å½•ã€‚
            user_agent: å¯é€‰ï¼Œä¸ºæ­¤æ¬¡è¯·æ±‚è®¾ç½® User-Agent å­—ç¬¦ä¸² (ç›®å‰ Playwright åœ¨ Context çº§åˆ«è®¾ç½®ï¼Œæ­¤å¤„ä¿ç•™ä»¥å¤‡å°†æ¥ä½¿ç”¨)ã€‚
            proxy_input: ä»£ç†é…ç½® (åŒ batch_download)ã€‚
            retries: å¯é€‰ï¼Œè¦†ç›– Downloader å®ä¾‹çš„ max_retries è®¾ç½®ã€‚
        """

        # 1. ç¡®å®šæœ€ç»ˆçš„ä¿å­˜è·¯å¾„ (output_path ä¼˜å…ˆ)
        final_output_path = output_path
        if not final_output_path:
            if use_remote_name:
                # çŒœæµ‹æ–‡ä»¶å
                guessed_filename = _guess_filename_from_url(url)
                final_output_path = os.path.join(
                    output_dir_for_remote_name, guessed_filename
                )
            else:
                # å¦‚æœä¸¤è€…éƒ½æ²¡æœ‰æŒ‡å®šï¼Œé»˜è®¤ä¿å­˜åˆ°å½“å‰ç›®å½•ï¼Œæ–‡ä»¶åæ ¹æ® URL çŒœæµ‹
                guessed_filename = _guess_filename_from_url(url)
                final_output_path = os.path.join("./", guessed_filename)

        # 2. å‡†å¤‡ä»»åŠ¡åˆ—è¡¨ (å•ä¸ªä»»åŠ¡)
        tasks = [
            (url, final_output_path, user_agent)
        ]  # ä»»åŠ¡ç»“æ„ (url, output_path, user_agent)

        logger.info(f"--- å¯åŠ¨å•ä¸ªé“¾æ¥ä¸‹è½½ä»»åŠ¡: {url} -> {final_output_path} ---")

        # 3. ä¸´æ—¶è¦†ç›–å¹¶å‘å’Œå»¶è¿Ÿï¼Œåˆ›å»ºä¸´æ—¶ Downloader å®ä¾‹æ¥æ‰§è¡Œ
        # ä½¿ç”¨ self çš„é…ç½®ï¼Œä½†å›ºå®šå¹¶å‘=1ï¼Œå»¶è¿Ÿ=(0.0, 0.0)
        temp_downloader = BrowserDownloader(
            headless=self.headless,
            timeout=self.timeout,
            delay_range=(0.0, 0.0),  # å•ä¸ªä»»åŠ¡ä¸éœ€è¦å»¶è¿Ÿ
            max_concurrency=1,  # å•ä¸ªä»»åŠ¡å›ºå®šå¹¶å‘ä¸º 1
            max_retries=retries if retries is not None else self.max_retries,
            # ç»§æ‰¿å›¾ç‰‡å‹ç¼©é…ç½®
            compress_quality=self.compress_quality,
            # quality_rule=self.quality_rule,
            # output_format=self.output_format,
            # remove_original=self.remove_original,
            # resize_threshold=self.resize_threshold,
            # fake_format=self.fake_format,
        )

        try:
            # 4. è¿è¡Œå¼‚æ­¥ä»»åŠ¡
            asyncio.run(temp_downloader._run_async(tasks, proxy_input))
            logger.info(f"å•ä¸ªé“¾æ¥ä¸‹è½½å®Œæˆ: {final_output_path}")
        except KeyboardInterrupt:
            logger.warning("ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­ã€‚")
        except Exception as e:
            logger.error(f"ä¸‹è½½ä»»åŠ¡å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")


# --- åŸå§‹çš„æµ‹è¯•å‡½æ•°ç°åœ¨ä½¿ç”¨ BrowserDownloader å®ä¾‹ ---


def test_batch_download():
    """æµ‹è¯•å¤šä¸ªé“¾æ¥æ‰¹é‡ä¸‹è½½çš„æƒ…å†µ"""
    test_url_1 = (
        "https://images.bike24.com/media/1020/i/mb/fc/0d/06/100048-00-d-163801.jpg"
    )
    test_url_2 = "https://covers-v2.ryefieldbooks.com/in-print-books/9783161491184"
    test_url_3 = "https://playwright.dev/"
    test_url_21 = "https://www.bigw.com.au/medias/sys_master/images/images/h6a/h5f/100887801561118.jpg"

    output_dir = "./browser_downloads_optimized"
    output_path_1 = os.path.join(output_dir, "bike24_image.jpg")
    output_path_2 = os.path.join(output_dir, "ryefieldbooks_cover.webp")
    output_path_3 = os.path.join(output_dir, "playwright_doc.html")
    output_path_21 = os.path.join(output_dir, "SK0000004-U-0.jpg")

    download_tasks = [
        (test_url_1, output_path_1),
        (test_url_2, output_path_2),
        (test_url_3, output_path_3),
        (test_url_21, output_path_21),
    ]

    # æ¸…ç†æ—§æ–‡ä»¶å’Œç›®å½•ä»¥ä¾¿æµ‹è¯•
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir, ignore_errors=True)
    os.makedirs(output_dir, exist_ok=True)

    # 1. å®ä¾‹åŒ– Downloaderï¼Œè®¾ç½®å…¬å…±é…ç½®
    downloader = BrowserDownloader(
        headless=False,
        max_concurrency=3,
        delay_range=(0.5, 1.0),
        max_retries=1,
        # quality_rule="",
    )

    print("--- å¯åŠ¨å¹¶å‘ä¸‹è½½ä»»åŠ¡ (æ— å¤´æ¨¡å¼ï¼Œå¹¶å‘=3ï¼ŒPage å¤ç”¨) ---")

    # 2. è°ƒç”¨å®ä¾‹æ–¹æ³•è¿›è¡Œä¸‹è½½
    downloader.batch_download(
        tasks=download_tasks,
        proxy_input="http://127.0.0.1:8800",
        output_dir=output_dir,  # å³ä½¿è¿™é‡ŒæŒ‡å®šäº† output_dirï¼Œä½†å› ä¸º tasks ä¸­æ˜¯å®Œæ•´è·¯å¾„ï¼Œå®ƒä¸ä¼šç”Ÿæ•ˆ
    )


def test_single_download():
    """æµ‹è¯•å•ä¸ªé“¾æ¥ä¸‹è½½çš„æƒ…å†µ"""
    test_url_1 = (
        "https://images.bike24.com/media/1020/i/mb/fc/0d/06/100048-00-d-163801.jpg"
    )
    test_url_4 = "https://www.baidu.com"
    output_dir = "./browser_downloads_single_test"

    if os.path.exists(output_dir):
        shutil.rmtree(output_dir, ignore_errors=True)
    os.makedirs(output_dir, exist_ok=True)

    # 1. å®ä¾‹åŒ– Downloaderï¼Œè®¾ç½®å…¬å…±é…ç½® (ä¾‹å¦‚ï¼Œå…¨å±€è®¾ç½® headless=True)
    downloader = BrowserDownloader(
        headless=False,  # å¯ä»¥å…¨å±€è®¾ç½®
        timeout=60,
    )

    print("\n\n=== ç¤ºä¾‹ 1: æŒ‡å®šå®Œæ•´ output_path (å›¾ç‰‡) ===")
    output_path_1 = os.path.join(output_dir, "bike24_test_011234.jpg")

    # 2. è°ƒç”¨å®ä¾‹æ–¹æ³•è¿›è¡Œä¸‹è½½
    downloader.single_download(
        url=test_url_1,
        output_path=output_path_1,
        # headless=False, # å¯ä»¥ç»§æ‰¿æˆ–è¦†ç›–
        # timeout=60, # ç»§æ‰¿æˆ–è¦†ç›–
    )

    print("\n\n=== ç¤ºä¾‹ 2: ä½¿ç”¨ use_remote_name çŒœæµ‹æ–‡ä»¶å (HTML) ===")
    downloader.single_download(
        url=test_url_4,
        use_remote_name=True,
        output_dir_for_remote_name=output_dir,
    )


# --- ç®€å•ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":
    test_single_download()
    # test_batch_download()
